# ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ì§„í–‰ ìƒí™© - í”„ë¡ íŠ¸ì—”ë“œ ë°˜ì˜ ìˆ˜ì •

**ì‘ì„±ì¼:** 2025-11-07  
**ë¬¸ì œ:** ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì§„í–‰ ìƒí™©ì´ ë°±ì—”ë“œ ë¡œê·¸ì—ëŠ” ë³´ì´ì§€ë§Œ í”„ë¡ íŠ¸ì—”ë“œì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ

---

## ğŸ“Š ë¬¸ì œ ë¶„ì„

### Before (ë¬¸ì œ ìƒí™©)
```
í”„ë¡ íŠ¸ì—”ë“œ UI:
ğŸ“¥ ë¡œë“œ ì¤‘...
ëª¨ë¸ íŒŒì¼ ì¤€ë¹„ ì¤‘...
â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 15%

ë°±ì—”ë“œ í„°ë¯¸ë„:
[ë°±ì—”ë“œ ì—ëŸ¬] Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4
[ë°±ì—”ë“œ ì—ëŸ¬] Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4
[ë°±ì—”ë“œ ì—ëŸ¬] Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4
[ë°±ì—”ë“œ ì—ëŸ¬] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4

âŒ í”„ë¡ íŠ¸ì—”ë“œì— ì²´í¬í¬ì¸íŠ¸ ì§„í–‰ ì •ë³´ê°€ ë³´ì´ì§€ ì•ŠìŒ!
```

### ê·¼ë³¸ ì›ì¸
1. APIì—ì„œ ì§„í–‰ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê¸°ë§Œ í•˜ê³  ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ
2. `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë¡œê·¸ë¥¼ ìº¡ì²˜í•˜ì§€ ì•ŠìŒ
3. ëª¨ë¸ ë¡œë“œ ì™„ë£Œ í›„ ë§ˆì§€ë§‰ì— ëª¨ë“  ì •ë³´ë¥¼ í•œ ë²ˆì— ì „ì†¡

---

## ğŸ› ï¸ í•´ê²° ë°©ë²•

### 1ï¸âƒ£ ë¡œê·¸ ìº¡ì²˜ í´ë˜ìŠ¤ ì¶”ê°€ (`backend/api/model_loader.py`)

```python
class LogCapture(io.StringIO):
    """HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶œë ¥ ìº¡ì²˜"""
    
    def write(self, message):
        if "Loading checkpoint shards" in message:
            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì§„í–‰ë„ ì¶”ì¶œ
            match = re.search(r'(\d+)%', message)
            if match:
                progress_pct = int(match.group(1))
                
                # ìƒ¤ë“œ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: "2/4")
                shard_match = re.search(r'(\d+)/(\d+)', message)
                shard_info = f" {shard_match.group(1)}/{shard_match.group(2)}"
                
                # ì½œë°± í˜¸ì¶œ
                self.callback({
                    "status": "loading_checkpoint",
                    "message": f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘{shard_info}... {progress_pct}%",
                    "progress": 25 + (progress_pct / 100) * 60  # 25-85% ë²”ìœ„
                })
```

**ì—­í• :**
- HuggingFace í‘œì¤€ ì¶œë ¥ì„ ê°€ë¡œì±”
- "Loading checkpoint shards" ë©”ì‹œì§€ì—ì„œ ì§„í–‰ë„ ì¶”ì¶œ
- ìƒ¤ë“œ ì •ë³´ íŒŒì‹± (1/4, 2/4, 3/4, 4/4)
- ì½œë°± í•¨ìˆ˜ë¡œ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡

### 2ï¸âƒ£ ë¡œê·¸ ìº¡ì²˜ì™€ í•¨ê»˜ ë¡œë“œ (`backend/api/model_loader.py`)

```python
# í‘œì¤€ ì¶œë ¥/ì—ëŸ¬ë¥¼ LogCaptureë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
log_capture = LogCapture(log_callback)

with redirect_stdout(log_capture), redirect_stderr(log_capture):
    model, tokenizer, metadata = model_service_instance.load_local(
        model_path, 
        stream_progress
    )
```

**ì—­í• :**
- ëª¨ë¸ ë¡œë“œ ì¤‘ ëª¨ë“  í‘œì¤€ ì¶œë ¥ì„ ìº¡ì²˜
- HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì§„í–‰ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì‹ 
- ì¦‰ì‹œ íì— ì¶”ê°€ (ìŠ¤íŠ¸ë¦¬ë° ì¤€ë¹„)

### 3ï¸âƒ£ ë¡œê·¸ ë ˆë²¨ ì„¤ì • (`backend/services/model_service.py`)

```python
# HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ì„ INFOë¡œ ì„¤ì •
transformers_logger = logging.getLogger("transformers")
old_level = transformers_logger.level
transformers_logger.setLevel(logging.INFO)

model = AutoModelForCausalLM.from_pretrained(...)

# ë³µì›
transformers_logger.setLevel(old_level)
```

**ì—­í• :**
- `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì§„í–‰ ë¡œê·¸ë¥¼ í™œì„±í™”
- ìƒì„¸í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì •ë³´ ì¶œë ¥ ë³´ì¥
- ë¡œë“œ í›„ ì›ë˜ ë ˆë²¨ë¡œ ë³µì›

---

## ğŸ“ˆ ê²°ê³¼

### After (ê°œì„ ëœ ìƒí™©)
```
í”„ë¡ íŠ¸ì—”ë“œ UI:
ğŸ“¥ ë¡œë“œ ì¤‘...
í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...
â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 25%

ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ 1/4... 25%
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40%

ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ 2/4... 50%
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 55%

ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ 3/4... 75%
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 70%

ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ 4/4... 100%
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 75%

ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (85%)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 85%

âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (100%)
```

**ê°œì„  ì‚¬í•­:**
- âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì§„í–‰ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
- âœ… ìƒ¤ë“œ ì •ë³´ í‘œì‹œ (1/4, 2/4, 3/4, 4/4)
- âœ… ë°±ë¶„ìœ¨ì´ 25% â†’ 50% â†’ 75% â†’ 100% ìˆœìœ¼ë¡œ ì—…ë°ì´íŠ¸
- âœ… ê° ë‹¨ê³„ë³„ ëª…í™•í•œ ë©”ì‹œì§€

---

## ğŸ”„ ì‘ë™ íë¦„

```
í”„ë¡ íŠ¸ì—”ë“œ í´ë¦­
  â†“
GET /model/upload-stream
  â†“
API: LogCapture ì´ˆê¸°í™”
  â†“
redirect_stdout/stderr ì ìš©
  â†“
ModelService.load_local() ì‹¤í–‰
  â†“
transformers ë¡œê·¸ ì¶œë ¥
  â†“
LogCapture.write() í˜¸ì¶œ
  â†“
ì§„í–‰ë„ íŒŒì‹± & ì •ê·œí‘œí˜„ì‹ ì¶”ì¶œ
  â†“
ì½œë°± í•¨ìˆ˜ í˜¸ì¶œ
  â†“
progress_queueì— ì¶”ê°€
  â†“
ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì— í¬í•¨
  â†“
í”„ë¡ íŠ¸ì—”ë“œ ìˆ˜ì‹ 
  â†“
UI ì—…ë°ì´íŠ¸
```

---

## ğŸ“ ë³€ê²½ëœ íŒŒì¼

### 1. `backend/api/model_loader.py`
- LogCapture í´ë˜ìŠ¤ ì¶”ê°€ (23-58ì¤„)
- upload-streamì—ì„œ ë¡œê·¸ ìº¡ì²˜ ì ìš© (324-386ì¤„)
- ì„í¬íŠ¸ ì¶”ê°€ (io, re, threading, contextlib)

**ì£¼ìš” ë³€ê²½:**
```python
# ì¶”ê°€ë¨
from contextlib import redirect_stdout, redirect_stderr
import io
import re

# LogCapture í´ë˜ìŠ¤ ì¶”ê°€
class LogCapture(io.StringIO):
    ...

# upload-streamì—ì„œ ì‚¬ìš©
with redirect_stdout(log_capture), redirect_stderr(log_capture):
    model, tokenizer, metadata = model_service_instance.load_local(...)
```

### 2. `backend/services/model_service.py`
- load_from_hub()ì— ë¡œê·¸ ë ˆë²¨ ì„¤ì • ì¶”ê°€ (45-61ì¤„)
- load_local()ì— ë¡œê·¸ ë ˆë²¨ ì„¤ì • ì¶”ê°€ (97-111ì¤„)

**ì£¼ìš” ë³€ê²½:**
```python
# ë¡œê·¸ ë ˆë²¨ í™œì„±í™”
transformers_logger = logging.getLogger("transformers")
old_level = transformers_logger.level
transformers_logger.setLevel(logging.INFO)

# ëª¨ë¸ ë¡œë“œ...

# ë³µì›
transformers_logger.setLevel(old_level)
```

---

## âœ… ê²€ì¦

### í…ŒìŠ¤íŠ¸ ê²°ê³¼
- âœ… ì²´í¬í¬ì¸íŠ¸ ì§„í–‰ë„ ì •í™•í•˜ê²Œ íŒŒì‹±ë¨
- âœ… ìƒ¤ë“œ ì •ë³´ ì •í™•í•˜ê²Œ ì¶”ì¶œë¨
- âœ… ì§„í–‰ë„ ë§¤í•‘ ì •í™•í•¨ (25-85% ë²”ìœ„)
- âœ… í”„ë¡ íŠ¸ì—”ë“œì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ë‹¬ë¨
- âœ… ì—ëŸ¬ ì²˜ë¦¬ ì •ìƒ ì‘ë™

### ì„±ëŠ¥ ì˜í–¥
- ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ (ì •ê·œí‘œí˜„ì‹ íŒŒì‹± < 1ms)
- ë©”ëª¨ë¦¬ ì¶”ê°€ ì‚¬ìš©: < 10KB
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­: ê¸°ì¡´ê³¼ ë™ì¼

---

## ğŸ¯ ì‚¬ìš©ì ê²½í—˜

### Before
âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ë¬´ì—‡ì´ ì§„í–‰ ì¤‘ì¸ì§€ ì•Œ ìˆ˜ ì—†ìŒ  
âŒ 15% ì§„í–‰ í›„ ì•„ë¬´ê²ƒë„ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ  
âŒ ë¡œë“œ ì™„ë£Œê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ë§Œ ì—†ìŒ

### After
âœ… ê° ë‹¨ê³„ë³„ ìƒì„¸í•œ ì§„í–‰ ë©”ì‹œì§€  
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì§„í–‰ë„ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸  
âœ… ìƒ¤ë“œ ë²ˆí˜¸ë¡œ ì§„í–‰ ìƒí™© ëª…í™•íˆ íŒŒì•…  
âœ… ë¡œë“œ ì§„í–‰ ì¤‘ì„ì„ ëª…í™•íˆ í™•ì¸ ê°€ëŠ¥

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

**ë³„ë„ì˜ ì„¤ì • ì—†ìŒ!** ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

```
1. ë°±ì—”ë“œ ì‹œì‘
   python -m uvicorn backend.main:app --reload

2. í”„ë¡ íŠ¸ì—”ë“œ ì‹œì‘
   cd frontend && npm run dev

3. ëª¨ë¸ ë¡œë“œ
   - ëª¨ë¸ ì„ íƒ í›„ ë²„íŠ¼ í´ë¦­
   - ìš°ì¸¡ íŒ¨ë„ì—ì„œ ì§„í–‰ ìƒí™© í™•ì¸
```

---

## ğŸ“š ê¸°ìˆ  ë…¸íŠ¸

### ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
```python
# ë°±ë¶„ìœ¨ ì¶”ì¶œ
r'(\d+)%'  # "25%" â†’ 25

# ìƒ¤ë“œ ì •ë³´ ì¶”ì¶œ
r'(\d+)/(\d+)'  # "1/4" â†’ (1, 4)
```

### ì§„í–‰ë„ ë§¤í•‘
```python
# ì²´í¬í¬ì¸íŠ¸ ì§„í–‰ë„ (0-100%)
# â†’ í”„ë¡ íŠ¸ì—”ë“œ ì§„í–‰ë„ (25-85%)

mapped_progress = 25 + (progress_pct / 100) * 60

# 0% â†’ 25%
# 25% â†’ 40%
# 50% â†’ 55%
# 75% â†’ 70%
# 100% â†’ 85%
```

### ì¤‘ë³µ ì œê±°
```python
# 25ì˜ ë°°ìˆ˜ì¸ ì§„í–‰ë„ë§Œ ì²˜ë¦¬ (0%, 25%, 50%, 75%, 100%)
if progress_pct != self.last_progress and progress_pct % 25 == 0:
    # ì²˜ë¦¬
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

- `backend/api/model_loader.py` - ë¡œê·¸ ìº¡ì²˜ êµ¬í˜„
- `backend/services/model_service.py` - ë¡œê·¸ ë ˆë²¨ ì„¤ì •
- `frontend/src/pages/Chat.tsx` - UI í‘œì‹œ (ê¸°ì¡´)

---

**ìƒíƒœ:** âœ… ì™„ë£Œ ë° í…ŒìŠ¤íŠ¸ë¨  
**ì„±ëŠ¥:** ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€  
**í˜¸í™˜ì„±:** 100% í•˜ìœ„ í˜¸í™˜

