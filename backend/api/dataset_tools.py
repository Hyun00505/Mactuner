"""ë°ì´í„°ì…‹ ë„êµ¬ API"""
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, File, HTTPException, UploadFile, Query
from pydantic import BaseModel, Field

from backend.services.dataset_service import DatasetService

router = APIRouter(tags=["dataset"])
dataset_service = DatasetService()

# ========================================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# ========================================


class DataUploadResponse(BaseModel):
    """ë°ì´í„° ì—…ë¡œë“œ ì‘ë‹µ"""

    status: str
    file_info: Dict[str, Any]  # ëª¨ë“  íƒ€ì… í—ˆìš©


class DataInfoResponse(BaseModel):
    """ë°ì´í„° ì •ë³´ ì‘ë‹µ"""

    shape: Dict
    size_mb: float
    dtypes: Dict[str, str]  # dtypeì„ ë¬¸ìì—´ë¡œ
    columns: List[str]


class CleaningRequest(BaseModel):
    """ë°ì´í„° ì •ì œ ìš”ì²­"""

    operation: str = Field(..., description="ì •ì œ ì‘ì—… (missing_values, duplicates, normalize_text, filter_by_length)")
    kwargs: Dict = Field(default_factory=dict, description="ì‘ì—…ë³„ íŒŒë¼ë¯¸í„°")


class EDASummary(BaseModel):
    """EDA ìš”ì•½"""

    total_rows: int
    total_columns: int
    memory_mb: float


class SplitRequest(BaseModel):
    """ë°ì´í„° ë¶„í•  ìš”ì²­"""

    test_size: float = Field(0.2, ge=0.01, le=0.99)
    random_state: int = Field(42)


class SaveRequest(BaseModel):
    """ë°ì´í„°ì…‹ ì €ì¥ ìš”ì²­"""

    filepath: str = Field(..., description="ì €ì¥ ê²½ë¡œ")
    format: str = Field("csv", description="ì €ì¥ í˜•ì‹ (csv, json, jsonl, excel)")
    encoding: Optional[str] = Field(None, description="íŒŒì¼ ì¸ì½”ë”© (utf-8, euc-kr, cp949 ë“±)")


class HFDownloadRequest(BaseModel):
    """HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìš”ì²­"""

    dataset_id: str = Field(..., description="HuggingFace ë°ì´í„°ì…‹ ID (ì˜ˆ: 'datasets/username/dataset_name')")
    hf_token: Optional[str] = Field(None, description="HuggingFace API í† í° (ì„ íƒì‚¬í•­)")
    split: str = Field("train", description="ë°ì´í„°ì…‹ split (train, validation, test ë“±)")
    max_samples: Optional[int] = Field(None, description="ë‹¤ìš´ë¡œë“œí•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")


# ========================================
# í—¬ìŠ¤ ì²´í¬
# ========================================


@router.get("/health", tags=["Health"])
async def dataset_health() -> Dict[str, str]:
    """ë°ì´í„°ì…‹ ë„êµ¬ í—¬ìŠ¤ ì²´í¬"""
    return {"status": "ok", "service": "dataset_tools"}


# ========================================
# ë°ì´í„° ì—…ë¡œë“œ
# ========================================


@router.post("/upload", response_model=DataUploadResponse)
async def upload_dataset(file: UploadFile = File(...), data_format: str = Query(None)) -> DataUploadResponse:
    """ë°ì´í„°ì…‹ íŒŒì¼ ì—…ë¡œë“œ - ìë™ í˜•ì‹ ê°ì§€"""
    try:
        if not file.filename:
            raise ValueError("íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")

        # íŒŒì¼ ì½ê¸°
        content = await file.read()

        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ
        original_filename = file.filename.lower()
        
        # í™•ì¥ì ê¸°ë°˜ í˜•ì‹ ê²°ì •
        if original_filename.endswith(".csv"):
            filename = f"upload.csv"
        elif original_filename.endswith(".json"):
            filename = f"upload.json"
        elif original_filename.endswith(".jsonl"):
            filename = f"upload.jsonl"
        elif original_filename.endswith(".xlsx") or original_filename.endswith(".xls"):
            filename = f"upload.xlsx"
        elif data_format and data_format in ("csv", "json", "jsonl", "excel"):
            # data_formatì´ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§„ ê²½ìš°
            ext = "xlsx" if data_format == "excel" else data_format
            filename = f"upload.{ext}"
        else:
            # ê¸°ë³¸ê°’
            filename = file.filename

        # ë°ì´í„°ì…‹ ë¡œë“œ
        df, file_info = dataset_service.load_dataset(content, filename)
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë¡œì»¬ íŒŒì¼) - ì¸ì½”ë”© ì •ë³´ í¬í•¨
        dataset_service.add_to_history(
            source="file", 
            filename=file.filename or filename,
            encoding=dataset_service.file_encoding
        )

        return DataUploadResponse(status="success", file_info=file_info)

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ========================================
# ë°ì´í„° ì¡°íšŒ
# ========================================


@router.get("/info")
async def get_data_info() -> Dict:
    """ë¡œë“œëœ ë°ì´í„° ì •ë³´ ì¡°íšŒ"""
    try:
        info = dataset_service.get_data_info()
        return {"status": "success", "data": info}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/preview")
async def get_preview(n_rows: int = Query(5)) -> Dict:
    """ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° - ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ì‘ë‹µ ë°˜í™˜"""
    try:
        preview = dataset_service.get_preview(n_rows)
        return {"status": "success", "data": preview}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/full-data")
async def get_full_data(limit: Optional[int] = Query(None), offset: int = Query(0)) -> Dict:
    """ì™„ì „í•œ ë°ì´í„° ë°˜í™˜ (í˜ì´ì§• ì§€ì›)"""
    try:
        # íŒŒë¼ë¯¸í„° ì•ˆì „ ì²˜ë¦¬
        safe_limit = None if limit is None else max(1, int(limit))
        safe_offset = max(0, int(offset))
        
        print(f"ğŸ“¥ [GET /full-data] limit={safe_limit}, offset={safe_offset}")
        data = dataset_service.get_full_data(limit=safe_limit, offset=safe_offset)
        print(f"âœ… [GET /full-data] ë°˜í™˜ ë°ì´í„°: {len(data.get('rows', []))} í–‰")
        return {"status": "success", "data": data}
    except ValueError as e:
        print(f"âŒ [GET /full-data] ValueError: {str(e)}")
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        print(f"âŒ [GET /full-data] Exception: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ì •ì œ
# ========================================


@router.post("/clean")
async def clean_data(request: CleaningRequest) -> Dict:
    """ë°ì´í„° ì •ì œ"""
    try:
        df = dataset_service.clean_data(
            cleaning_type=request.operation,
            **request.kwargs
        )
        return {"status": "success", "message": f"ì •ì œ ì™„ë£Œ: {len(df)} í–‰"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# í† í° ë¶„ì„
# ========================================


@router.post("/analyze-tokens")
async def analyze_tokens(
    column: str = Query(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼"),
    model_name: str = Query("gpt2", description="í† í¬ë‚˜ì´ì € ëª¨ë¸ ì´ë¦„"),
) -> Dict:
    """í† í° ê¸¸ì´ ë¶„ì„"""
    try:
        result = dataset_service.analyze_token_length(column, model_name)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)
# ========================================


@router.get("/cached-datasets")
async def get_cached_datasets() -> Dict:
    """HuggingFace ìºì‹œì— ìˆëŠ” ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹ ëª©ë¡"""
    try:
        import os
        from pathlib import Path
        
        # HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬
        hf_cache = Path.home() / ".cache" / "huggingface" / "datasets"
        
        if not hf_cache.exists():
            return {"status": "success", "data": []}
        
        cached_datasets = []
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë°ì´í„°ì…‹ í´ë” íƒìƒ‰
        for dataset_dir in hf_cache.iterdir():
            if dataset_dir.is_dir():
                dataset_id = dataset_dir.name
                # datasets--username--dataset-name í˜•ì‹ì„ username/dataset-nameìœ¼ë¡œ ë³€í™˜
                if dataset_id.startswith("datasets--"):
                    parts = dataset_id.replace("datasets--", "").split("--")
                    if len(parts) >= 2:
                        dataset_id = "/".join(parts)
                
                # ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
                size_bytes = sum(
                    f.stat().st_size
                    for f in dataset_dir.rglob("*")
                    if f.is_file()
                )
                size_mb = size_bytes / (1024 ** 2)
                
                cached_datasets.append({
                    "dataset_id": dataset_id,
                    "cache_dir": str(dataset_dir),
                    "size_mb": round(size_mb, 2),
                })
        
        return {"status": "success", "data": cached_datasets}
    except Exception as e:
        return {"status": "error", "data": [], "message": str(e)}


@router.get("/history")
async def get_dataset_history() -> Dict:
    """ë°ì´í„°ì…‹ ë¡œë“œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        history = dataset_service.get_history()
        return {"status": "success", "data": history}
    except Exception as e:
        return {"status": "error", "data": [], "message": str(e)}


@router.post("/history/reload/{index}")
async def reload_from_history(index: int) -> Dict:
    """íˆìŠ¤í† ë¦¬ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œ"""
    try:
        history = dataset_service.get_history()
        if index < 0 or index >= len(history):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤")
        
        item = history[index]
        
        # ì†ŒìŠ¤ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        if item["source"] == "hf":
            # HuggingFace ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œ
            result = dataset_service.download_hf_dataset(
                dataset_id=item.get("hf_dataset_id"),
                hf_token=item.get("hf_token"),
                split=item.get("hf_split", "train"),
                max_samples=item.get("hf_max_samples"),
            )
            return result
        else:
            return {"status": "info", "message": "íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì…‹ì€ ì¬ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/history/clear")
async def clear_history(delete_data: bool = False) -> Dict:
    """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    try:
        result = dataset_service.clear_history(delete_data=delete_data)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/history/{index}")
async def delete_history_item(index: int, delete_data: bool = False) -> Dict:
    """íŠ¹ì • íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ"""
    try:
        result = dataset_service.delete_history_item(index, delete_data=delete_data)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/statistics")
async def get_statistics() -> Dict:
    """ê¸°ë³¸ í†µê³„ - ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ì‘ë‹µ ë°˜í™˜"""
    try:
        stats = dataset_service.get_statistics()
        return {"status": "success", "data": stats}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/missing-values")
async def get_missing_values() -> Dict:
    """ê²°ì¸¡ì¹˜ ë¶„ì„"""
    try:
        missing = dataset_service.get_missing_values()
        return {"status": "success", "data": missing}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/value-distribution")
async def get_value_distribution(
    column: str = Query(..., description="ë¶„ì„í•  ì»¬ëŸ¼"),
    top_n: int = Query(10, ge=1, le=100),
) -> Dict:
    """ê°’ ë¶„í¬ ë¶„ì„"""
    try:
        dist = dataset_service.get_value_distribution(column, top_n)
        return {"status": "success", "data": dist}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/correlation")
async def get_correlation() -> Dict:
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    try:
        corr = dataset_service.get_correlation()
        return {"status": "success", "data": corr}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/summary")
async def get_eda_summary() -> Dict:
    """EDA ì¢…í•© ìš”ì•½"""
    try:
        stats = dataset_service.get_statistics()
        missing = dataset_service.get_missing_values()

        return {
            "status": "success",
            "summary": {
                "total_rows": stats["total_rows"],
                "total_columns": stats["total_columns"],
                "memory_mb": stats["memory_mb"],
                "missing_values": missing["total_missing_values"],
                "columns": dataset_service.data.columns.tolist(),
            },
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ë¶„í• 
# ========================================


@router.post("/split")
async def split_dataset(request: SplitRequest) -> Dict:
    """Train/Test ë°ì´í„° ë¶„í• """
    try:
        train_df, test_df = dataset_service.train_test_split(
            test_size=request.test_size, random_state=request.random_state
        )

        return {
            "status": "success",
            "train_rows": len(train_df),
            "test_rows": len(test_df),
            "train_ratio": len(train_df) / (len(train_df) + len(test_df)),
            "test_ratio": len(test_df) / (len(train_df) + len(test_df)),
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ì €ì¥
# ========================================


@router.post("/save")
async def save_dataset(request: SaveRequest) -> Dict:
    """ë°ì´í„°ì…‹ ì €ì¥"""
    try:
        result = dataset_service.save_dataset(request.filepath, request.format, request.encoding)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ì´ˆê¸°í™”
# ========================================


@router.post("/reset")
async def reset_data() -> Dict[str, str]:
    """ë°ì´í„°ë¥¼ ì›ë³¸ìœ¼ë¡œ ë³µì›"""
    try:
        result = dataset_service.reset_data()
        return {**result, "status": "success"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
# ========================================


@router.post("/download-hf")
async def download_hf_dataset(request: HFDownloadRequest) -> Dict:
    """HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
    try:
        result = dataset_service.download_hf_dataset(
            dataset_id=request.dataset_id,
            hf_token=request.hf_token,
            split=request.split,
            max_samples=request.max_samples,
        )
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ - ì¸ì½”ë”© ì •ë³´ í¬í•¨
        dataset_service.add_to_history(
            source="hf",
            hf_dataset_id=request.dataset_id,
            hf_token=request.hf_token,
            hf_split=request.split,
            hf_max_samples=request.max_samples,
            encoding=dataset_service.file_encoding,
        )
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
