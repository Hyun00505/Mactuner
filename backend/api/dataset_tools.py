"""ë°ì´í„°ì…‹ ë„êµ¬ API"""
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, File, HTTPException, UploadFile, Query
from pydantic import BaseModel, Field

from backend.services.dataset_service import DatasetService

router = APIRouter(tags=["dataset"])
dataset_service = DatasetService()

# ========================================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# ========================================


class DataUploadResponse(BaseModel):
    """ë°ì´í„° ì—…ë¡œë“œ ì‘ë‹µ"""

    status: str
    file_info: Dict[str, Any]  # ëª¨ë“  íƒ€ì… í—ˆìš©


class DataInfoResponse(BaseModel):
    """ë°ì´í„° ì •ë³´ ì‘ë‹µ"""

    shape: Dict
    size_mb: float
    dtypes: Dict[str, str]  # dtypeì„ ë¬¸ìì—´ë¡œ
    columns: List[str]


class CleaningRequest(BaseModel):
    """ë°ì´í„° ì •ì œ ìš”ì²­"""

    operation: str = Field(..., description="ì •ì œ ì‘ì—… (missing_values, duplicates, normalize_text, filter_by_length)")
    kwargs: Dict = Field(default_factory=dict, description="ì‘ì—…ë³„ íŒŒë¼ë¯¸í„°")


class EDASummary(BaseModel):
    """EDA ìš”ì•½"""

    total_rows: int
    total_columns: int
    memory_mb: float


class SplitRequest(BaseModel):
    """ë°ì´í„° ë¶„í•  ìš”ì²­"""

    test_size: float = Field(0.2, ge=0.01, le=0.99)
    random_state: int = Field(42)


class SaveRequest(BaseModel):
    """ë°ì´í„°ì…‹ ì €ì¥ ìš”ì²­"""

    filepath: str = Field(..., description="ì €ì¥ ê²½ë¡œ")
    format: str = Field("csv", description="ì €ì¥ í˜•ì‹ (csv, json, jsonl, excel)")
    encoding: Optional[str] = Field(None, description="íŒŒì¼ ì¸ì½”ë”© (utf-8, euc-kr, cp949 ë“±)")


class HFDownloadRequest(BaseModel):
    """HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìš”ì²­"""

    dataset_id: str = Field(..., description="HuggingFace ë°ì´í„°ì…‹ ID (ì˜ˆ: 'datasets/username/dataset_name')")
    hf_token: Optional[str] = Field(None, description="HuggingFace API í† í° (ì„ íƒì‚¬í•­)")
    split: str = Field("train", description="ë°ì´í„°ì…‹ split (train, validation, test ë“±)")
    max_samples: Optional[int] = Field(None, description="ë‹¤ìš´ë¡œë“œí•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")


# ========================================
# í—¬ìŠ¤ ì²´í¬
# ========================================


@router.get("/health", tags=["Health"])
async def dataset_health() -> Dict[str, str]:
    """ë°ì´í„°ì…‹ ë„êµ¬ í—¬ìŠ¤ ì²´í¬"""
    return {"status": "ok", "service": "dataset_tools"}


# ========================================
# ë°ì´í„° ì—…ë¡œë“œ
# ========================================


@router.post("/upload", response_model=DataUploadResponse)
async def upload_dataset(file: UploadFile = File(...), data_format: str = Query(None)) -> DataUploadResponse:
    """ë°ì´í„°ì…‹ íŒŒì¼ ì—…ë¡œë“œ - ìë™ í˜•ì‹ ê°ì§€"""
    try:
        if not file.filename:
            raise ValueError("íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")

        # íŒŒì¼ ì½ê¸°
        content = await file.read()

        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ
        original_filename = file.filename.lower()
        
        # í™•ì¥ì ê¸°ë°˜ í˜•ì‹ ê²°ì •
        if original_filename.endswith(".csv"):
            filename = f"upload.csv"
        elif original_filename.endswith(".json"):
            filename = f"upload.json"
        elif original_filename.endswith(".jsonl"):
            filename = f"upload.jsonl"
        elif original_filename.endswith(".xlsx") or original_filename.endswith(".xls"):
            filename = f"upload.xlsx"
        elif data_format and data_format in ("csv", "json", "jsonl", "excel"):
            # data_formatì´ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§„ ê²½ìš°
            ext = "xlsx" if data_format == "excel" else data_format
            filename = f"upload.{ext}"
        else:
            # ê¸°ë³¸ê°’
            filename = file.filename

        # ë°ì´í„°ì…‹ ë¡œë“œ
        df, file_info = dataset_service.load_dataset(content, filename)
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë¡œì»¬ íŒŒì¼) - ì¸ì½”ë”© ì •ë³´ í¬í•¨
        dataset_service.add_to_history(
            source="file", 
            filename=file.filename or filename,
            encoding=dataset_service.file_encoding
        )

        return DataUploadResponse(status="success", file_info=file_info)

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ========================================
# ë°ì´í„° ì¡°íšŒ
# ========================================


@router.get("/info")
async def get_data_info() -> Dict:
    """ë¡œë“œëœ ë°ì´í„° ì •ë³´ ì¡°íšŒ"""
    try:
        info = dataset_service.get_data_info()
        return {"status": "success", "data": info}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/preview")
async def get_preview(n_rows: int = Query(5)) -> Dict:
    """ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° - ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ì‘ë‹µ ë°˜í™˜"""
    try:
        preview = dataset_service.get_preview(n_rows)
        return {"status": "success", "data": preview}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/full-data")
async def get_full_data(limit: Optional[int] = Query(None), offset: int = Query(0)) -> Dict:
    """ì™„ì „í•œ ë°ì´í„° ë°˜í™˜ (í˜ì´ì§• ì§€ì›)"""
    try:
        # íŒŒë¼ë¯¸í„° ì•ˆì „ ì²˜ë¦¬
        safe_limit = None if limit is None else max(1, int(limit))
        safe_offset = max(0, int(offset))
        
        print(f"ğŸ“¥ [GET /full-data] limit={safe_limit}, offset={safe_offset}")
        data = dataset_service.get_full_data(limit=safe_limit, offset=safe_offset)
        print(f"âœ… [GET /full-data] ë°˜í™˜ ë°ì´í„°: {len(data.get('rows', []))} í–‰")
        return {"status": "success", "data": data}
    except ValueError as e:
        print(f"âŒ [GET /full-data] ValueError: {str(e)}")
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì—ëŸ¬ ëŒ€ì‹ )
        return {"status": "no_data", "data": {"rows": [], "total_rows": 0, "columns": [], "dtypes": {}}, "message": str(e)}
    except Exception as e:
        print(f"âŒ [GET /full-data] Exception: {str(e)}")
        import traceback
        traceback.print_exc()
        # ì—ëŸ¬ ëŒ€ì‹  ë¹ˆ ê²°ê³¼ ë°˜í™˜
        return {"status": "no_data", "data": {"rows": [], "total_rows": 0, "columns": [], "dtypes": {}}, "message": str(e)}


# ========================================
# ë°ì´í„° ì •ì œ
# ========================================


@router.post("/clean")
async def clean_data(request: CleaningRequest) -> Dict:
    """ë°ì´í„° ì •ì œ"""
    try:
        df = dataset_service.clean_data(
            cleaning_type=request.operation,
            **request.kwargs
        )
        return {"status": "success", "message": f"ì •ì œ ì™„ë£Œ: {len(df)} í–‰"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# í† í° ë¶„ì„
# ========================================


@router.post("/analyze-tokens")
async def analyze_tokens(
    column: str = Query(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼"),
    model_name: str = Query("gpt2", description="í† í¬ë‚˜ì´ì € ëª¨ë¸ ì´ë¦„"),
) -> Dict:
    """í† í° ê¸¸ì´ ë¶„ì„"""
    try:
        result = dataset_service.analyze_token_length(column, model_name)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)
# ========================================


@router.get("/cached-datasets")
async def get_cached_datasets() -> Dict:
    """HuggingFace ìºì‹œì— ìˆëŠ” ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹ ëª©ë¡"""
    try:
        import os
        from pathlib import Path
        
        # HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬
        hf_cache = Path.home() / ".cache" / "huggingface" / "datasets"
        
        if not hf_cache.exists():
            return {"status": "success", "data": []}
        
        cached_datasets = []
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë°ì´í„°ì…‹ í´ë” íƒìƒ‰
        for dataset_dir in hf_cache.iterdir():
            if dataset_dir.is_dir():
                dataset_id = dataset_dir.name
                # datasets--username--dataset-name í˜•ì‹ì„ username/dataset-nameìœ¼ë¡œ ë³€í™˜
                if dataset_id.startswith("datasets--"):
                    parts = dataset_id.replace("datasets--", "").split("--")
                    if len(parts) >= 2:
                        dataset_id = "/".join(parts)
                
                # ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
                size_bytes = sum(
                    f.stat().st_size
                    for f in dataset_dir.rglob("*")
                    if f.is_file()
                )
                size_mb = size_bytes / (1024 ** 2)
                
                cached_datasets.append({
                    "dataset_id": dataset_id,
                    "cache_dir": str(dataset_dir),
                    "size_mb": round(size_mb, 2),
                })
        
        return {"status": "success", "data": cached_datasets}
    except Exception as e:
        return {"status": "error", "data": [], "message": str(e)}


@router.get("/history")
async def get_dataset_history() -> Dict:
    """ë°ì´í„°ì…‹ ë¡œë“œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        history = dataset_service.get_history()
        return {"status": "success", "data": history}
    except Exception as e:
        return {"status": "error", "data": [], "message": str(e)}


@router.post("/history/reload/{index}")
async def reload_from_history(index: int) -> Dict:
    """íˆìŠ¤í† ë¦¬ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œ"""
    try:
        history = dataset_service.get_history()
        if index < 0 or index >= len(history):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤")
        
        item = history[index]
        
        # ì†ŒìŠ¤ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        if item["source"] == "hf":
            # HuggingFace ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œ
            result = dataset_service.download_hf_dataset(
                dataset_id=item.get("hf_dataset_id"),
                hf_token=item.get("hf_token"),
                split=item.get("hf_split", "train"),
                max_samples=item.get("hf_max_samples"),
            )
            return result
        else:
            return {"status": "info", "message": "íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì…‹ì€ ì¬ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/history/clear")
async def clear_history(delete_data: bool = False) -> Dict:
    """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    try:
        result = dataset_service.clear_history(delete_data=delete_data)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/history/{index}")
async def delete_history_item(index: int, delete_data: bool = False) -> Dict:
    """íŠ¹ì • íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ"""
    try:
        result = dataset_service.delete_history_item(index, delete_data=delete_data)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/statistics")
async def get_statistics() -> Dict:
    """ê¸°ë³¸ í†µê³„ - ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ì‘ë‹µ ë°˜í™˜"""
    try:
        stats = dataset_service.get_statistics()
        return {"status": "success", "data": stats}
    except ValueError as e:
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        return {"status": "no_data", "data": None, "message": str(e)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/missing-values")
async def get_missing_values() -> Dict:
    """ê²°ì¸¡ì¹˜ ë¶„ì„"""
    try:
        missing = dataset_service.get_missing_values()
        return {"status": "success", "data": missing}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/value-distribution")
async def get_value_distribution(
    column: str = Query(..., description="ë¶„ì„í•  ì»¬ëŸ¼"),
    top_n: int = Query(10, ge=1, le=100),
) -> Dict:
    """ê°’ ë¶„í¬ ë¶„ì„"""
    try:
        dist = dataset_service.get_value_distribution(column, top_n)
        return {"status": "success", "data": dist}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/correlation")
async def get_correlation() -> Dict:
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    try:
        corr = dataset_service.get_correlation()
        return {"status": "success", "data": corr}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/eda/summary")
async def get_eda_summary() -> Dict:
    """EDA ì¢…í•© ìš”ì•½"""
    try:
        stats = dataset_service.get_statistics()
        missing = dataset_service.get_missing_values()

        return {
            "status": "success",
            "summary": {
                "total_rows": stats["total_rows"],
                "total_columns": stats["total_columns"],
                "memory_mb": stats["memory_mb"],
                "missing_values": missing["total_missing_values"],
                "columns": dataset_service.data.columns.tolist(),
            },
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ë¶„í• 
# ========================================


@router.post("/split")
async def split_dataset(request: SplitRequest) -> Dict:
    """Train/Test ë°ì´í„° ë¶„í• """
    try:
        train_df, test_df = dataset_service.train_test_split(
            test_size=request.test_size, random_state=request.random_state
        )

        return {
            "status": "success",
            "train_rows": len(train_df),
            "test_rows": len(test_df),
            "train_ratio": len(train_df) / (len(train_df) + len(test_df)),
            "test_ratio": len(test_df) / (len(train_df) + len(test_df)),
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ì €ì¥
# ========================================


@router.post("/save")
async def save_dataset(request: SaveRequest) -> Dict:
    """ë°ì´í„°ì…‹ ì €ì¥"""
    try:
        result = dataset_service.save_dataset(request.filepath, request.format, request.encoding)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë°ì´í„° ì´ˆê¸°í™”
# ========================================


@router.post("/reset")
async def reset_data() -> Dict[str, str]:
    """ë°ì´í„°ë¥¼ ì›ë³¸ìœ¼ë¡œ ë³µì›"""
    try:
        result = dataset_service.reset_data()
        return {**result, "status": "success"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
# ========================================


@router.post("/download-hf")
async def download_hf_dataset(request: HFDownloadRequest) -> Dict:
    """HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
    try:
        result = dataset_service.download_hf_dataset(
            dataset_id=request.dataset_id,
            hf_token=request.hf_token,
            split=request.split,
            max_samples=request.max_samples,
        )
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ - ì¸ì½”ë”© ì •ë³´ í¬í•¨
        dataset_service.add_to_history(
            source="hf",
            hf_dataset_id=request.dataset_id,
            hf_token=request.hf_token,
            hf_split=request.split,
            hf_max_samples=request.max_samples,
            encoding=dataset_service.file_encoding,
        )
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========================================
# ë¡œì»¬ ë°ì´í„°ì…‹ ëª©ë¡
# ========================================


@router.get("/local-datasets")
async def get_local_datasets() -> Dict:
    """ë¡œì»¬ì— ìˆëŠ” ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ (íˆìŠ¤í† ë¦¬ ê¸°ë°˜)"""
    try:
        from pathlib import Path
        import json
        
        datasets = []
        history_file = Path(__file__).parent.parent.parent / "data" / "dataset_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, "r") as f:
                    history_data = json.load(f)
                
                # íˆìŠ¤í† ë¦¬ í˜•ì‹ ì²˜ë¦¬ (ë°°ì—´ ë˜ëŠ” ê°ì²´)
                history_list = []
                if isinstance(history_data, list):
                    # ë°°ì—´ í˜•ì‹
                    history_list = history_data
                elif isinstance(history_data, dict) and "history" in history_data:
                    # ê°ì²´ with "history" í‚¤ í˜•ì‹
                    history_list = history_data["history"]
                
                # íˆìŠ¤í† ë¦¬ì—ì„œ ë°ì´í„°ì…‹ ì •ë³´ ì¶”ì¶œ
                for item in history_list:
                    # íŒŒì¼ ì—…ë¡œë“œ
                    if item.get("source") in ["file", "upload"]:
                        datasets.append({
                            "dataset_id": item.get("filename") or item.get("file_name", "Unknown"),
                            "source": "upload",
                            "format": item.get("format", "unknown"),
                            "timestamp": item.get("timestamp", ""),
                            "size_mb": item.get("size_mb", 0),
                            "rows": item.get("rows", 0),
                            "columns": item.get("columns", 0),
                        })
                    # HuggingFace ë°ì´í„°ì…‹
                    elif item.get("source") == "hf":
                        datasets.append({
                            "dataset_id": item.get("hf_dataset_id", "Unknown"),
                            "source": "huggingface",
                            "split": item.get("hf_split", "train"),
                            "timestamp": item.get("timestamp", ""),
                            "rows": item.get("rows", 0),
                            "columns": item.get("columns", 0),
                            "format": item.get("format", "HuggingFace Dataset"),
                        })
                
                print(f"âœ… ë¡œì»¬ ë°ì´í„°ì…‹ ì¡°íšŒ ì™„ë£Œ: {len(datasets)}ê°œ ë°œê²¬")
            except Exception as e:
                print(f"âš ï¸ íˆìŠ¤í† ë¦¬ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        else:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ íŒŒì¼ ì—†ìŒ: {history_file}")
        
        return {"status": "success", "datasets": datasets}
    
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ë°ì´í„°ì…‹ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"status": "success", "datasets": []}  # ì—ëŸ¬ë„ ë¹ˆ ë°°ì—´ ë°˜í™˜


@router.post("/load-by-id")
async def load_dataset_by_id(request: Dict) -> Dict:
    """ë°ì´í„°ì…‹ IDë¡œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    try:
        dataset_id = request.get("dataset_id")
        if not dataset_id:
            raise ValueError("dataset_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        from pathlib import Path
        import json
        
        history_file = Path(__file__).parent.parent.parent / "data" / "dataset_history.json"
        
        if not history_file.exists():
            raise ValueError("íˆìŠ¤í† ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        with open(history_file, "r") as f:
            history_data = json.load(f)
        
        # íˆìŠ¤í† ë¦¬ í˜•ì‹ ì²˜ë¦¬
        history_list = []
        if isinstance(history_data, list):
            history_list = history_data
        elif isinstance(history_data, dict) and "history" in history_data:
            history_list = history_data["history"]
        
        # ë°ì´í„°ì…‹ IDë¡œ íˆìŠ¤í† ë¦¬ì—ì„œ ì°¾ê¸°
        dataset_item = None
        for item in history_list:
            # íŒŒì¼ ì—…ë¡œë“œ
            if item.get("source") in ["file", "upload"]:
                if item.get("filename") == dataset_id or item.get("file_name") == dataset_id:
                    dataset_item = item
                    break
            # HuggingFace ë°ì´í„°ì…‹
            elif item.get("source") == "hf":
                if item.get("hf_dataset_id") == dataset_id:
                    dataset_item = item
                    break
        
        if not dataset_item:
            raise ValueError(f"ë°ì´í„°ì…‹ ID '{dataset_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì†ŒìŠ¤ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        if dataset_item.get("source") == "hf":
            # HuggingFace ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œ
            result = dataset_service.download_hf_dataset(
                dataset_id=dataset_item.get("hf_dataset_id"),
                hf_token=dataset_item.get("hf_token"),
                split=dataset_item.get("hf_split", "train"),
                max_samples=dataset_item.get("hf_max_samples"),
            )
            return {"status": "success", "message": f"ë°ì´í„°ì…‹ '{dataset_id}' ë¡œë“œ ì™„ë£Œ", "data": result}
        else:
            # íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì…‹ì€ íŒŒì¼ ê²½ë¡œì—ì„œ ë¡œë“œ
            # íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ (íŒŒì¼ì€ ë©”ëª¨ë¦¬ì—ë§Œ ìˆìœ¼ë¯€ë¡œ ì¬ì—…ë¡œë“œ í•„ìš”)
            file_path = dataset_item.get("file_path") or dataset_item.get("filename") or dataset_item.get("file_name")
            if not file_path:
                raise ValueError("íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì…‹ì€ ì¬ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # íŒŒì¼ ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if not Path(file_path).is_absolute():
                # data í´ë”ì—ì„œ ì°¾ê¸°
                data_dir = Path(__file__).parent.parent.parent / "data"
                file_path = data_dir / file_path
            
            if not Path(file_path).exists():
                raise ValueError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}. íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì…‹ì€ ì¬ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # íŒŒì¼ ì½ê¸°
            with open(file_path, "rb") as f:
                file_content = f.read()
            
            # ë°ì´í„°ì…‹ ë¡œë“œ
            result = dataset_service.load_dataset(file_content, Path(file_path).name)
            return {"status": "success", "message": f"ë°ì´í„°ì…‹ '{dataset_id}' ë¡œë“œ ì™„ë£Œ", "data": result[1]}
    
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
