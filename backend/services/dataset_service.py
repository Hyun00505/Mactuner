"""ë°ì´í„°ì…‹ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
import io
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import chardet
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer

from backend.config import settings
import json


class DatasetService:
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ë¶„ì„ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.data: Optional[pd.DataFrame] = None
        self.original_data: Optional[pd.DataFrame] = None
        self.file_info: Dict[str, Any] = {}
        self.file_encoding: str = "utf-8"  # ê°ì§€ëœ ì¸ì½”ë”©
        self.detected_format: str = "unknown"  # ê°ì§€ëœ í¬ë§·
        self.dataset_history: List[Dict[str, Any]] = []  # ë°ì´í„°ì…‹ ë¡œë“œ íˆìŠ¤í† ë¦¬
        self.history_file = Path(settings.DATA_DIR) / "dataset_history.json"  # íˆìŠ¤í† ë¦¬ ì €ì¥ íŒŒì¼
        self._load_history_from_file()  # ì‹œì‘í•  ë•Œ íˆìŠ¤í† ë¦¬ ë¡œë“œ

    def detect_encoding(self, content: bytes) -> str:
        """íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€"""
        try:
            detected = chardet.detect(content)
            encoding = detected.get("encoding", "utf-8")
            confidence = detected.get("confidence", 0)
            
            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ utf-8 ì‚¬ìš©
            if confidence < 0.7:
                return "utf-8"
            
            return encoding if encoding else "utf-8"
        except Exception:
            return "utf-8"

    def load_dataset(
        self, file_content: bytes, filename: str
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            # ì¸ì½”ë”© ê°ì§€
            self.file_encoding = self.detect_encoding(file_content)
            
            if filename.endswith(".csv"):
                self.detected_format = "CSV"
                self.data = pd.read_csv(io.BytesIO(file_content), encoding=self.file_encoding)
            elif filename.endswith(".json"):
                self.detected_format = "JSON"
                self.data = pd.read_json(io.BytesIO(file_content))
            elif filename.endswith(".jsonl"):
                self.detected_format = "JSONL"
                self.data = pd.read_json(io.BytesIO(file_content), lines=True)
            elif filename.endswith(".xlsx") or filename.endswith(".xls"):
                self.detected_format = "Excel"
                # Excel íŒŒì¼ ì²˜ë¦¬
                self.data = pd.read_excel(io.BytesIO(file_content))
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {filename}")

            # ì›ë³¸ ë°ì´í„° ë°±ì—…
            self.original_data = self.data.copy()

            # dtypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ í•¨
            dtypes_dict = {}
            for col, dtype in self.data.dtypes.items():
                dtypes_dict[col] = str(dtype)

            # íŒŒì¼ ì •ë³´ ì €ì¥
            self.file_info = {
                "filename": filename,
                "size_bytes": len(file_content),
                "size_mb": float(len(file_content) / (1024**2)),
                "rows": len(self.data),
                "columns": self.data.columns.tolist(),
                "dtypes": dtypes_dict,
                "encoding": self.file_encoding,  # ê°ì§€ëœ ì¸ì½”ë”©
                "format": self.detected_format,  # ê°ì§€ëœ í¬ë§·
            }

            return self.data, self.file_info

        except Exception as e:
            raise RuntimeError(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def get_data_info(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # dtypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ í•¨
        dtypes_dict = {}
        for col, dtype in self.data.dtypes.items():
            dtypes_dict[col] = str(dtype)

        return {
            "shape": {"rows": len(self.data), "columns": len(self.data.columns)},
            "size_mb": float(self.data.memory_usage(deep=True).sum() / (1024**2)),
            "dtypes": dtypes_dict,
            "columns": self.data.columns.tolist(),
        }

    def get_preview(self, n_rows: int = 5) -> Dict[str, Any]:
        """ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return {
            "head": self.data.head(n_rows).to_dict(orient="records"),
            "tail": self.data.tail(n_rows).to_dict(orient="records"),
            "total_rows": len(self.data),
            "preview_rows": n_rows * 2,
        }

    # ========================================
    # ë°ì´í„° ì •ì œ
    # ========================================

    def clean_data(self, cleaning_type: str, **kwargs) -> Dict[str, Any]:
        """í†µí•© ë°ì´í„° ì •ì œ"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if cleaning_type == "missing_values":
            return self.handle_missing_values(**kwargs)
        elif cleaning_type == "duplicates":
            return self.remove_duplicates(**kwargs)
        elif cleaning_type == "normalize_text":
            return self.normalize_text(**kwargs)
        elif cleaning_type == "filter_by_length":
            return self.filter_by_text_length(**kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì •ì œ ì‘ì—…: {cleaning_type}")

    def handle_missing_values(
        self, strategy: str = "drop", value: Optional[Any] = None
    ) -> Dict[str, Any]:
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        missing_before = self.data.isnull().sum().sum()

        if strategy == "drop":
            self.data = self.data.dropna()
        elif strategy == "fill":
            self.data = self.data.fillna(value or 0)
        elif strategy == "forward_fill":
            self.data = self.data.fillna(method="ffill")
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì „ëµ: {strategy}")

        missing_after = self.data.isnull().sum().sum()

        return {
            "status": "success",
            "operation": "missing_values",
            "strategy": strategy,
            "missing_before": int(missing_before),
            "missing_after": int(missing_after),
            "removed_rows": max(0, len(self.original_data) - len(self.data)),
            "remaining_rows": len(self.data),
        }

    def remove_duplicates(self, subset: Optional[List[str]] = None) -> Dict[str, int]:
        """ì¤‘ë³µ í–‰ ì œê±°"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        rows_before = len(self.data)
        self.data = self.data.drop_duplicates(subset=subset)
        rows_after = len(self.data)

        return {
            "status": "success",
            "operation": "duplicates",
            "rows_before": rows_before,
            "rows_after": rows_after,
            "duplicates_removed": rows_before - rows_after,
            "remaining_rows": rows_after,
        }

    def normalize_text(self, columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” - LLM íŒŒì¸íŠœë‹ìš©"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if columns is None:
            # ë¬¸ìì—´ ì»¬ëŸ¼ ìë™ ê°ì§€
            columns = self.data.select_dtypes(include=["object"]).columns.tolist()

        changes = {}
        for col in columns:
            if col not in self.data.columns:
                continue

            # ì›ë³¸ ì €ì¥
            original_col = self.data[col].copy()

            # ì†Œë¬¸ì ë³€í™˜
            self.data[col] = self.data[col].str.lower()
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ê¸°ë³¸ ì˜ì–´/ìˆ«ìë§Œ ìœ ì§€)
            self.data[col] = self.data[col].str.replace(r"[^a-z0-9\s]", "", regex=True)
            # ì—°ì† ê³µë°± ì œê±°
            self.data[col] = self.data[col].str.replace(r"\s+", " ", regex=True)
            # ê³µë°± ì œê±°
            self.data[col] = self.data[col].str.strip()

            changes[col] = "ì •ê·œí™” ì™„ë£Œ"

        return {
            "status": "success",
            "operation": "normalize_text",
            "normalized_columns": changes,
            "total_normalized": len(changes),
            "remaining_rows": len(self.data),
        }

    def filter_by_text_length(
        self, column: str = None, min_length: int = 10, max_length: int = 10000
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ í•„í„°ë§ - LLM íŒŒì¸íŠœë‹ìš©"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ì»¬ëŸ¼ ìë™ ì„ íƒ (ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì‚¬ìš©)
        if column is None:
            text_columns = self.data.select_dtypes(include=["object"]).columns.tolist()
            if not text_columns:
                raise ValueError("í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            column = text_columns[0]

        if column not in self.data.columns:
            raise ValueError(f"ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {column}")

        rows_before = len(self.data)

        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
        self.data = self.data[
            (self.data[column].str.len() >= min_length)
            & (self.data[column].str.len() <= max_length)
        ]

        rows_after = len(self.data)

        return {
            "status": "success",
            "operation": "filter_by_length",
            "column": column,
            "min_length": min_length,
            "max_length": max_length,
            "rows_before": rows_before,
            "rows_after": rows_after,
            "rows_removed": rows_before - rows_after,
            "remaining_rows": rows_after,
        }

    def analyze_token_length(
        self, text_column: str, model_name: str = "gpt2"
    ) -> Dict[str, Any]:
        """í† í° ê¸¸ì´ ë¶„ì„"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
        except Exception as e:
            raise RuntimeError(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

        token_lengths = self.data[text_column].apply(
            lambda x: len(tokenizer.encode(str(x))) if pd.notna(x) else 0
        )

        return {
            "model": model_name,
            "column": text_column,
            "min_tokens": int(token_lengths.min()),
            "max_tokens": int(token_lengths.max()),
            "mean_tokens": float(token_lengths.mean()),
            "median_tokens": float(token_lengths.median()),
            "std_tokens": float(token_lengths.std()),
        }

    # ========================================
    # íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
    # ========================================

    def get_statistics(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í†µê³„"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # dtypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        dtypes_str = {str(k): str(v) for k, v in self.data.dtypes.value_counts().to_dict().items()}
        describe_dict = self.data.describe().to_dict()
        
        # describe ê²°ê³¼ì˜ NaNì„ ì²˜ë¦¬
        for col in describe_dict:
            for key in describe_dict[col]:
                if isinstance(describe_dict[col][key], float):
                    if np.isnan(describe_dict[col][key]):
                        describe_dict[col][key] = None

        stats = {
            "total_rows": len(self.data),
            "total_columns": len(self.data.columns),
            "memory_mb": float(self.data.memory_usage(deep=True).sum() / (1024**2)),
            "dtypes": dtypes_str,
            "describe": describe_dict,
        }

        return stats

    def get_missing_values(self) -> Dict[str, Any]:
        """ê²°ì¸¡ì¹˜ ë¶„ì„"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        missing = self.data.isnull().sum()
        missing_percent = (missing / len(self.data) * 100).round(2)

        return {
            "columns_with_missing": missing[missing > 0].to_dict(),
            "missing_percentage": missing_percent[missing_percent > 0].to_dict(),
            "total_missing_values": int(missing.sum()),
            "total_cells": int(len(self.data) * len(self.data.columns)),
        }

    def get_value_distribution(self, column: str, top_n: int = 10) -> Dict[str, Any]:
        """ê°’ ë¶„í¬ ë¶„ì„"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if column not in self.data.columns:
            raise ValueError(f"ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {column}")

        value_counts = self.data[column].value_counts().head(top_n)

        return {
            "column": column,
            "unique_values": int(self.data[column].nunique()),
            "top_values": value_counts.to_dict(),
            "total_count": len(self.data),
        }

    def get_correlation(self) -> Dict[str, Any]:
        """ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìƒê´€ê´€ê³„"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        numeric_data = self.data.select_dtypes(include=[np.number])

        if len(numeric_data.columns) == 0:
            return {"message": "ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}

        correlation = numeric_data.corr()
        
        # NaNì„ Noneìœ¼ë¡œ ë³€í™˜
        corr_dict = correlation.to_dict()
        for col in corr_dict:
            for key in corr_dict[col]:
                if isinstance(corr_dict[col][key], float):
                    if np.isnan(corr_dict[col][key]):
                        corr_dict[col][key] = None

        return {
            "correlation": corr_dict,
            "numeric_columns": numeric_data.columns.tolist(),
        }

    # ========================================
    # ë°ì´í„° ë¶„í• 
    # ========================================

    def train_test_split(
        self, test_size: float = 0.2, random_state: int = 42
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Train/Test ë¶„í• """
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if not 0 < test_size < 1:
            raise ValueError("test_sizeëŠ” 0ê³¼ 1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        indices = np.arange(len(self.data))
        np.random.seed(random_state)
        np.random.shuffle(indices)

        split_point = int(len(self.data) * (1 - test_size))
        train_indices = indices[:split_point]
        test_indices = indices[split_point:]

        train_data = self.data.iloc[train_indices].reset_index(drop=True)
        test_data = self.data.iloc[test_indices].reset_index(drop=True)

        return train_data, test_data

    def save_dataset(self, filepath: str, format: str = "csv", encoding: str = None) -> Dict[str, str]:
        """ë°ì´í„°ì…‹ ì €ì¥"""
        if self.data is None:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        filepath = Path(filepath).expanduser()
        
        # ì¸ì½”ë”© ì„¤ì • (ê¸°ë³¸ê°’: ì›ë˜ íŒŒì¼ì˜ ì¸ì½”ë”© ë˜ëŠ” utf-8)
        save_encoding = encoding or self.file_encoding or "utf-8"

        try:
            if format == "csv":
                self.data.to_csv(filepath, index=False, encoding=save_encoding)
            elif format == "json":
                self.data.to_json(filepath, orient="records", force_ascii=False)
            elif format == "jsonl":
                self.data.to_json(filepath, orient="records", lines=True, force_ascii=False)
            elif format == "excel":
                self.data.to_excel(filepath, index=False, sheet_name="Data")
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")

            return {
                "status": "success",
                "filepath": str(filepath),
                "rows": len(self.data),
                "columns": len(self.data.columns),
                "format": format,
                "encoding": save_encoding,
                "size_mb": float(filepath.stat().st_size / (1024**2)),
            }

        except Exception as e:
            raise RuntimeError(f"ë°ì´í„°ì…‹ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def reset_data(self) -> Dict[str, str]:
        """ë°ì´í„° ì´ˆê¸°í™” (ì›ë³¸ìœ¼ë¡œ ë³µì›)"""
        if self.original_data is None:
            raise ValueError("ì›ë³¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        self.data = self.original_data.copy()
        return {"status": "success", "message": "ë°ì´í„°ê°€ ì›ë³¸ìœ¼ë¡œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    def _load_demo_dataset(self, dataset_name: str, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ì…‹ ë¡œë“œ (ì¸í„°ë„· ë¶ˆí•„ìš”)"""
        import random
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        num_rows = max_samples or 1000
        
        if dataset_name.lower() == "test":
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°
            data = {
                "id": list(range(1, num_rows + 1)),
                "text": [f"ìƒ˜í”Œ í…ìŠ¤íŠ¸ {i}" for i in range(1, num_rows + 1)],
                "label": [random.choice(["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"]) for _ in range(num_rows)],
                "score": [round(random.uniform(0, 1), 2) for _ in range(num_rows)],
            }
        elif dataset_name.lower() == "demo":
            # ë”ë¯¸ í•œêµ­ì–´ ë°ì´í„°
            data = {
                "í•œê¸€_ì œëª©": [f"ì œëª©_{i}" for i in range(1, num_rows + 1)],
                "ë‚´ìš©": [f"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. í–‰ ë²ˆí˜¸: {i}" for i in range(1, num_rows + 1)],
                "ì¹´í…Œê³ ë¦¬": [random.choice(["ë‰´ìŠ¤", "ë¸”ë¡œê·¸", "SNS", "ë¦¬ë·°"]) for _ in range(num_rows)],
                "ì¢‹ì•„ìš”": [random.randint(0, 1000) for _ in range(num_rows)],
                "ëŒ“ê¸€_ìˆ˜": [random.randint(0, 500) for _ in range(num_rows)],
            }
        else:
            # ì¼ë°˜ ë”ë¯¸ ë°ì´í„°
            data = {
                "index": list(range(1, num_rows + 1)),
                "value_a": [random.uniform(0, 100) for _ in range(num_rows)],
                "value_b": [random.uniform(0, 100) for _ in range(num_rows)],
                "category": [random.choice(["A", "B", "C"]) for _ in range(num_rows)],
            }
        
        # DataFrame ìƒì„±
        self.data = pd.DataFrame(data)
        self.original_data = self.data.copy()
        
        # dtypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        dtypes_dict = {}
        for col, dtype in self.data.dtypes.items():
            dtypes_dict[col] = str(dtype)
        
        # íŒŒì¼ ì •ë³´ ì €ì¥
        self.file_encoding = "utf-8"
        self.detected_format = "Demo Dataset"
        
        self.file_info = {
            "filename": f"{dataset_name}_demo.parquet",
            "rows": len(self.data),
            "columns": self.data.columns.tolist(),
            "dtypes": dtypes_dict,
            "encoding": self.file_encoding,
            "format": self.detected_format,
            "dataset_id": f"demo/{dataset_name}",
            "split": "train",
        }
        
        return {
            "status": "success",
            "message": f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ! (ë”ë¯¸ ë°ì´í„°)",
            "dataset_info": self.file_info,
            "file_info": self.file_info,
        }

    def download_hf_dataset(
        self,
        dataset_id: str,
        hf_token: Optional[str] = None,
        split: str = "train",
        max_samples: Optional[int] = None,
    ) -> Dict[str, Any]:
        """HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ (ë¡œì»¬ ìºì‹œ ìë™ í™œìš©)"""
        try:
            import logging
            from pathlib import Path
            logger = logging.getLogger(__name__)
            
            # dataset_id ì •ê·œí™”
            dataset_id = dataset_id.strip()
            
            # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ì…‹
            if dataset_id.lower() in ["test", "demo", "example"]:
                return self._load_demo_dataset(dataset_id, max_samples)
            
            logger.info(f"ğŸ”„ HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ ì‹œì‘: {dataset_id}")
            
            # HuggingFaceì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ (ìë™ ìºì‹±)
            # cache_dirì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë¡œì»¬ ìºì‹œ í™œìš©
            hf_cache_dir = Path.home() / ".cache" / "huggingface" / "datasets"
            
            kwargs = {
                "cache_dir": str(hf_cache_dir),  # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ ëª…ì‹œ
                "trust_remote_code": True,  # ì›ê²© ì½”ë“œ ì‹ ë¢°
            }
            if hf_token:
                kwargs["token"] = hf_token
            
            try:
                logger.info(f"ğŸ“¥ ë¡œì»¬ ìºì‹œ í™•ì¸: {hf_cache_dir}")
                dataset = load_dataset(dataset_id, split=split, **kwargs)
                logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ (ìºì‹œ í™œìš©: {hf_cache_dir})")
            except Exception as first_error:
                # ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨ ì‹œ, ì—¬ëŸ¬ í˜•ì‹ì„ ì‹œë„
                alternate_ids = []
                
                # username/name í˜•ì‹ì´ë©´ datasets/ prefix ì¶”ê°€ ì‹œë„
                if "/" in dataset_id and not dataset_id.startswith("datasets/"):
                    alternate_ids.append(f"datasets/{dataset_id}")
                
                # ë‹¤ë¥¸ split ì‹œë„ (ì²« ë²ˆì§¸ ì‹œë„ê°€ split ë¬¸ì œì¼ ìˆ˜ ìˆìŒ)
                alternate_splits = ["validation", "test", None]
                
                dataset_loaded = False
                last_error = first_error
                
                # ë‹¤ë¥¸ í˜•ì‹ ë° split ì‹œë„
                for alt_id in alternate_ids:
                    for alt_split in alternate_splits:
                        try:
                            logger.info(f"Trying: {alt_id} with split: {alt_split}")
                            if alt_split:
                                dataset = load_dataset(alt_id, split=alt_split, **kwargs)
                            else:
                                # split ì—†ì´ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
                                dataset_full = load_dataset(alt_id, **kwargs)
                                # ì²« ë²ˆì§¸ split ì‚¬ìš©
                                if isinstance(dataset_full, dict):
                                    dataset = dataset_full[list(dataset_full.keys())[0]]
                                else:
                                    dataset = dataset_full
                            
                            dataset_id = alt_id
                            dataset_loaded = True
                            logger.info(f"âœ… Successfully loaded: {alt_id}")
                            break
                        except Exception as e:
                            last_error = e
                            continue
                    
                    if dataset_loaded:
                        break
                
                if not dataset_loaded:
                    logger.error(f"All attempts failed. Last error: {last_error}")
                    raise first_error
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if max_samples and len(dataset) > max_samples:
                dataset = dataset.select(range(max_samples))
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            self.data = dataset.to_pandas()
            self.original_data = self.data.copy()
            
            # dtypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            dtypes_dict = {}
            for col, dtype in self.data.dtypes.items():
                dtypes_dict[col] = str(dtype)
            
            # íŒŒì¼ ì •ë³´ ì €ì¥
            self.file_encoding = "utf-8"
            self.detected_format = "HuggingFace Dataset"
            
            self.file_info = {
                "filename": f"{dataset_id.split('/')[-1]}.parquet",
                "rows": len(self.data),
                "columns": self.data.columns.tolist(),
                "dtypes": dtypes_dict,
                "encoding": self.file_encoding,
                "format": self.detected_format,
                "dataset_id": dataset_id,
                "split": split,
            }
            
            return {
                "status": "success",
                "message": f"âœ… HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!",
                "dataset_info": self.file_info,
                "file_info": self.file_info,
            }
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Dataset loading failed: {error_msg}")
            
            # ë” ì¹œì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
            if "doesn't exist" in error_msg or "cannot be accessed" in error_msg:
                detail = (
                    f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"â€¢ ë°ì´í„°ì…‹ ID: {dataset_id}\n"
                    f"â€¢ Split: {split}\n\n"
                    f"ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\n"
                    f"1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (HuggingFace Hub ì ‘ê·¼ í•„ìš”)\n"
                    f"2. ì •í™•í•œ ID ì…ë ¥ (ì˜ˆ: username/dataset-name)\n"
                    f"3. Split ì´ë¦„ í™•ì¸ (train, validation, test ë“±)\n"
                    f"4. Private ë°ì´í„°ì…‹ì´ë©´ í† í° ì…ë ¥\n"
                    f"5. HuggingFace ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸"
                )
                raise RuntimeError(detail)
            elif "Couldn't reach" in error_msg or "Failed to resolve" in error_msg or "Network" in error_msg:
                detail = (
                    f"âŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜\n\n"
                    f"HuggingFace Hubì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"â€¢ ë°ì´í„°ì…‹ ID: {dataset_id}\n\n"
                    f"ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                    f"1. ì¸í„°ë„· ì—°ê²° í™•ì¸\n"
                    f"2. ë°©í™”ë²½/VPN ì„¤ì • í™•ì¸\n"
                    f"3. HuggingFace ì„œë²„ ìƒíƒœ í™•ì¸"
                )
                raise RuntimeError(detail)
            else:
                raise RuntimeError(f"HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
    
    def _convert_to_serializable(self, obj: Any) -> Any:
        """Numpy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._convert_to_serializable(val) for key, val in obj.items()}
        else:
            return str(obj)
    
    def add_to_history(self, source: str = "file", **kwargs) -> None:
        """ë°ì´í„°ì…‹ íˆìŠ¤í† ë¦¬ì— í•­ëª© ì¶”ê°€"""
        import datetime
        
        if not self.file_info:
            return
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ ì •ë³´ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ê¸°ì¡´ í•­ëª© ì œê±°)
        self.dataset_history = [
            h for h in self.dataset_history
            if h.get("filename") != self.file_info.get("filename")
        ]
        
        # file_infoì˜ ëª¨ë“  ê°’ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        safe_file_info = {
            key: self._convert_to_serializable(value)
            for key, value in self.file_info.items()
        }
        
        # kwargsì˜ ëª¨ë“  ê°’ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (None ì œì™¸)
        safe_kwargs = {
            key: self._convert_to_serializable(value)
            for key, value in kwargs.items()
            if value is not None
        }
        
        history_item = {
            "id": len(self.dataset_history) + 1,
            "source": source,  # "file", "hf", "demo"
            "filename": str(safe_file_info.get("filename", "unknown")),
            "rows": int(safe_file_info.get("rows", 0)),
            "columns": int(len(safe_file_info.get("columns", []))),
            "size_mb": float(safe_file_info.get("size_mb", 0)),
            "format": str(safe_file_info.get("format", "unknown")),
            "timestamp": datetime.datetime.now().isoformat(),
            **safe_kwargs,  # dataset_id, hf_token ë“± ì¶”ê°€ ì •ë³´ (ëª¨ë‘ ì•ˆì „í•¨)
        }
        
        # ìµœì‹  í•­ëª©ì„ ë¦¬ìŠ¤íŠ¸ ì•ì— ì¶”ê°€
        self.dataset_history.insert(0, history_item)
        
        # ìµœëŒ€ 10ê°œ í•­ëª©ë§Œ ìœ ì§€
        self.dataset_history = self.dataset_history[:10]
        
        # íŒŒì¼ì— ì €ì¥
        self._save_history_to_file()
    
    def get_history(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ì…‹ ë¡œë“œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        return self.dataset_history
    
    def _load_history_from_file(self) -> None:
        """íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        try:
            if self.history_file.exists():
                with open(self.history_file, "r", encoding="utf-8") as f:
                    self.dataset_history = json.load(f)
        except Exception as e:
            print(f"íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.dataset_history = []
    
    def _save_history_to_file(self) -> None:
        """íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            self.history_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.history_file, "w", encoding="utf-8") as f:
                json.dump(self.dataset_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def clear_history(self, delete_data: bool = False) -> Dict[str, Any]:
        """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
        result = {
            "status": "success",
            "message": "íˆìŠ¤í† ë¦¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": len(self.dataset_history),
        }
        
        if delete_data:
            result["message"] = "íˆìŠ¤í† ë¦¬ì™€ ë°ì´í„°ê°€ ëª¨ë‘ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            self.data = None
            self.original_data = None
            self.file_info = {}
        
        self.dataset_history = []
        self._save_history_to_file()
        
        return result
    
    def delete_history_item(self, index: int, delete_data: bool = False) -> Dict[str, Any]:
        """íŠ¹ì • íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ"""
        if index < 0 or index >= len(self.dataset_history):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤")
        
        deleted_item = self.dataset_history.pop(index)
        self._save_history_to_file()
        
        message = f"{deleted_item.get('filename', 'Unknown')} íˆìŠ¤í† ë¦¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        
        if delete_data:
            message = f"{deleted_item.get('filename', 'Unknown')} íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            # í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ê°€ ì´ í•­ëª©ê³¼ ê°™ìœ¼ë©´ ì‚­ì œ
            if self.file_info and self.file_info.get("filename") == deleted_item.get("filename"):
                self.data = None
                self.original_data = None
                self.file_info = {}
        
        return {
            "status": "success",
            "message": message,
            "deleted_item": deleted_item,
        }
