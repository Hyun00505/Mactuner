# 🧪 체크포인트 진행 상황 표시 - 테스트 가이드

**목적:** 모델 로드 시 체크포인트 진행 상황이 프론트엔드에서 제대로 표시되는지 검증

---

## 📋 사전 준비

### 1. 환경 확인

```bash
# 백엔드 디렉토리 확인
cd /Users/kimhyunbin/Desktop/github_bunhine0452/Mactuner

# 가상환경 활성화 (필요한 경우)
source venv/bin/activate  # Mac/Linux
# or
venv\Scripts\activate  # Windows
```

### 2. 의존성 확인

```bash
# 필요한 패키지 설치
pip install -r backend/requirements.txt

# 또는 개별 설치
pip install transformers torch fastapi uvicorn
```

### 3. 테스트 모델 준비

```bash
# 로컬 models 폴더 확인
ls -la models/

# 테스트에 사용할 모델 (작은 모델 추천):
# - meta-llama/Llama-2-7b-hf
# - mistralai/Mistral-7B-v0.1
# - gpt2
# - TinyLlama/TinyLlama-1.1B
```

---

## 🚀 테스트 시작

### 1단계: 백엔드 시작

```bash
# 터미널 1: 백엔드 시작
cd /Users/kimhyunbin/Desktop/github_bunhine0452/Mactuner

python -m uvicorn backend.main:app --reload --host 0.0.0.0 --port 8001
```

**예상 출력:**

```
INFO:     Uvicorn running on http://0.0.0.0:8001
INFO:     Application startup complete
```

### 2단계: 프론트엔드 시작

```bash
# 터미널 2: 프론트엔드 시작
cd /Users/kimhyunbin/Desktop/github_bunhine0452/Mactuner/frontend

npm run dev
```

**예상 출력:**

```
VITE v... build ready in ... ms

➜  Local:   http://localhost:5173/
```

### 3단계: 브라우저 열기

```
http://localhost:5173/
```

---

## ✅ 테스트 체크리스트

### T1: UI 초기 상태

- [ ] Chat 페이지가 정상 로드됨
- [ ] 우측 설정 패널에 "🤖 모델 선택" 섹션이 보임
- [ ] 모델 목록이 표시됨

### T2: 모델 로드 시작

```
✓ 단계:
1. 우측 패널에서 테스트 모델 선택
2. 모델 버튼 클릭
3. 로드 시작 메시지 확인
```

**예상 화면:**

```
📥 로드 중...
모델 파일 준비 중...
█░░░░░░░░░░░░░░░░░░ 15%
```

- [ ] "📥 로드 중..." 메시지 표시됨
- [ ] 진행 바가 보임
- [ ] 백분율이 표시됨

### T3: 토크나이저 로드 단계

```
예상 진행 시간: 1-3초
```

**예상 화면:**

```
📥 로드 중...
토크나이저 로드 중...
████░░░░░░░░░░░░░░░░ 25%
```

- [ ] "토크나이저 로드 중..." 메시지 표시됨
- [ ] 진행 바가 업데이트됨
- [ ] 백분율이 25%로 표시됨

### T4: 모델 로드 단계 (체크포인트)

```
예상 진행 시간: 10-30초 (모델 크기에 따라 다름)
```

**예상 화면:**

```
📥 로드 중...
체크포인트 로드 중 1/4... 25%
█████████░░░░░░░░░░░░ 40%

체크포인트 로드 중 2/4... 50%
██████████████░░░░░░░ 55%

체크포인트 로드 중 3/4... 75%
██████████████████░░░░ 70%

체크포인트 로드 중 4/4... 100%
████████████████████░░ 75%
```

**확인 사항:**

- [ ] "체크포인트 로드 중" 메시지가 표시됨
- [ ] 샤드 정보가 표시됨 (1/4, 2/4, 3/4, 4/4)
- [ ] 진행 바가 부드럽게 업데이트됨
- [ ] 진행도 비율이 상향 추세임

### T5: 마무리 단계

```
예상 진행 시간: 2-5초
```

**예상 화면:**

```
📥 로드 중...
모델을 MPS로 이동...
████████████████░░░░░ 80%

메타데이터 추출 완료
██████████████████░░░░ 90%

✅ 모델 로드 완료!
████████████████████░░ 100%
```

- [ ] "모델을 MPS로 이동..." 메시지 표시됨
- [ ] "메타데이터 추출 완료" 메시지 표시됨
- [ ] 최종 "✅ 모델 로드 완료!" 메시지 표시됨
- [ ] 진행 바가 100%로 도달

### T6: 로드 완료 후

- [ ] 로드 상태 박스가 사라짐
- [ ] 모델명이 "✅ {모델명}" 으로 표시됨
- [ ] 채팅 입력창이 활성화됨
- [ ] 채팅을 시작할 수 있음

---

## 🖥️ 백엔드 터미널 검증

### 로그 확인 포인트

#### 1. 초기 로그

```
INFO: 127.0.0.1:52470 - "POST /model/upload-stream HTTP/1.1" 200 OK
```

#### 2. 토크나이저 로드 로그

```
tokenizer_config.json: 100%|████████| ...
special_tokens_map.json: 100%|████████| ...
```

#### 3. 체크포인트 로드 로그 (중요!)

```
[백엔드 에러] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[백엔드 에러] Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.00s/it]
[백엔드 에러] Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.48s/it]
[백엔드 에러] Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.08s/it]
[백엔드 에러] Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  6.16s/it]
```

**확인 사항:**

- [ ] "Loading checkpoint shards" 메시지가 출력됨
- [ ] 진행도가 0%, 25%, 50%, 75%, 100% 순서로 증가
- [ ] 샤드 정보가 표시됨 (0/4, 1/4, 2/4, 3/4, 4/4)

#### 4. 메타데이터 로그

```
INFO: config.json loaded from ...
```

---

## 🐛 문제 해결

### 문제 1: 진행 상황이 전혀 보이지 않음

**확인 사항:**

1. 백엔드 프로세스가 실행 중인가?

   ```bash
   lsof -i :8001  # 포트 8001 확인
   ```

2. 프론트엔드에서 API 호출이 성공했는가?

   ```
   브라우저 개발자 도구 → Network 탭
   /model/upload-stream 요청 확인
   상태: 200 OK
   ```

3. 응답 스트림이 나타나는가?
   ```
   Network 탭 → /model/upload-stream → Response 탭
   JSON 스트림이 보여야 함
   ```

**해결 방법:**

```bash
# 백엔드 재시작
Ctrl + C  # 중지
python -m uvicorn backend.main:app --reload
```

### 문제 2: 진행 상황이 중간에 멈춤

**원인:** 모델 로드 중 오류 발생

**해결 방법:**

1. 백엔드 로그 확인

   ```
   [백엔드 에러] RuntimeError: ... 로 시작하는 오류 찾기
   ```

2. 터미널 전체 메시지 복사
3. 스택 트레이스 확인

### 문제 3: "Loading checkpoint shards" 로그가 보이지 않음

**원인:** 로그 레벨이 너무 높음

**확인 방법:**

```bash
# backend/services/model_service.py 에서:
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.INFO)  # ← 이 부분 확인
```

### 문제 4: 진행도가 잘못 계산됨

**예:** 25%, 50% 만 나타나고 75%, 100% 가 안 보임

**원인:** 정규표현식 패턴 문제

**확인 방법:**

```python
# backend/api/model_loader.py 의 LogCapture 클래스 확인
match = re.search(r'(\d+)%', message)  # ← 이 패턴 확인
```

---

## 📊 성능 측정

### 측정 항목

#### 1. 로드 시간

```
시작 시간: 로드 버튼 클릭
종료 시간: "✅ 모델 로드 완료!" 표시

측정값: 종료 - 시작
```

**예상 범위:**

- 작은 모델 (1-3GB): 15-30초
- 중간 모델 (7-13GB): 30-60초
- 큰 모델 (30GB+): 60초 이상

#### 2. 진행도 정확도

```
예상:
- 25%, 50%, 75%, 100% 순서로 표시
- 각 진행도 사이 시간 간격이 유사

확인:
- 프론트엔드 UI에서 확인
- 백엔드 로그에서 시간 확인
```

#### 3. 프론트엔드 응답성

```
- 진행 메시지 업데이트 지연 < 500ms
- UI 렌더링 부드러움
- 스크롤/클릭 반응성 유지
```

---

## 📸 스크린샷 확인 목록

### 로드 시작 화면

```
□ 타이머 시작
□ "📥 로드 중..." 메시지 보임
□ 진행 바 보임
□ "모델 파일 준비 중..." 메시지
```

### 토크나이저 로드

```
□ "토크나이저 로드 중..." 메시지
□ 진행도 25%
```

### 체크포인트 로드 (중요!)

```
□ "체크포인트 로드 중 1/4... 25%"
□ 진행도 40%
□
□ "체크포인트 로드 중 2/4... 50%"
□ 진행도 55%
□
□ "체크포인트 로드 중 3/4... 75%"
□ 진행도 70%
□
□ "체크포인트 로드 중 4/4... 100%"
□ 진행도 75%
```

### 최종 완료

```
□ "✅ 모델 로드 완료!" 메시지
□ 진행도 100%
□ 채팅 입력창 활성화
```

---

## 💾 테스트 결과 기록

### 테스트 1: 기본 동작 확인

```
날짜: ________
모델: ________
시간: ________ 초
결과: ✓ / ✗
메모: ___________________
```

### 테스트 2: 여러 모델 테스트

```
모델 1: ________
- 진행 메시지: ✓ / ✗
- 진행도 업데이트: ✓ / ✗
- 최종 완료: ✓ / ✗

모델 2: ________
- 진행 메시지: ✓ / ✗
- 진행도 업데이트: ✓ / ✗
- 최종 완료: ✓ / ✗
```

### 테스트 3: 에러 상황

```
시나리오: 로드 중 네트워크 끊김
결과: ✓ / ✗
메모: ___________________
```

---

## 🎯 최종 검증

모든 항목이 체크되었으면 테스트 완료! ✅

```
필수 항목:
□ 초기 로드 상태 메시지 표시
□ 토크나이저 로드 메시지 표시
□ 체크포인트 로드 메시지 표시 (1/4, 2/4, 3/4, 4/4)
□ 진행도 정확하게 업데이트
□ 최종 완료 메시지 표시
□ 완료 후 채팅 기능 정상 작동

보너스 항목:
□ 진행 바 부드러운 애니메이션
□ 백분율 정확함
□ 백엔드 로그 명확함
□ 성능 영향 무시할 수 있는 수준
```

---

## 📞 지원 요청

문제가 해결되지 않으면:

1. **백엔드 로그 수집**

   ```bash
   # 전체 로그 저장
   python -m uvicorn backend.main:app --reload 2>&1 | tee backend_log.txt
   ```

2. **브라우저 개발자 도구 로그**

   ```
   F12 → Console → 모든 메시지 복사
   ```

3. **네트워크 분석**

   ```
   F12 → Network → /model/upload-stream → Response 확인
   ```

4. **정보 제공**
   - backend_log.txt
   - console 메시지
   - /model/upload-stream 응답
   - 사용 중인 모델 정보

---

**테스트 완료 후 결과를 공유해주세요! 🚀**
