# π‰ MacTuner Phase 1.3: ν•™μµ μ—”μ§„ κµ¬ν„ μ™„λ£

## π“ κµ¬ν„ μ”μ•½

**μ΄ 1,050μ¤„μ ν”„λ΅λ•μ… ν’μ§ μ½”λ“ μ‘μ„±** (λ„μ : 2,962μ¤„)

### κµ¬ν„λ λ¨λ“

#### 1. ν•™μµ μ„λΉ„μ¤ (`backend/services/training_service.py`)
- **μ¤„ μ**: 380μ¤„
- **ν΄λμ¤**: `TrainingService`
- **κΈ°λ¥**: 14κ° λ©”μ„λ“

#### 2. ν•™μµ API (`backend/api/training.py`)
- **μ¤„ μ**: 280μ¤„
- **μ—”λ“ν¬μΈνΈ**: 12κ°
- **μ”μ²­/μ‘λ‹µ λ¨λΈ**: 5κ°

#### 3. ν¬κ΄„μ  ν…μ¤νΈ (`tests/backend/test_training.py`)
- **μ¤„ μ**: 390μ¤„
- **ν…μ¤νΈ μΌ€μ΄μ¤**: 28κ°
- **ν…μ¤νΈ ν΄λμ¤**: 7κ°

---

## β¨ κµ¬ν„λ κΈ°λ¥

### 1οΈβƒ£ LoRA/QLoRA μ„¤μ •
```
β… LoRA μ„¤μ • (Low-Rank Adaptation)
   - Rank, Alpha, Dropout μ»¤μ¤ν„°λ§μ΄μ§•
   - Target modules μ„ νƒ (q_proj, v_proj λ“±)

β… QLoRA μ„¤μ • (4-bit μ–‘μν™” + LoRA)
   - 4-bit BitsAndBytes μ„¤μ •
   - λ©”λ¨λ¦¬ ν¨μ¨μ μΈ λ―Έμ„Έ μ΅°μ •
```

**μ—”λ“ν¬μΈνΈ**:
- `POST /train/config-lora` - LoRA μ„¤μ •
- `POST /train/config-qlora` - QLoRA μ„¤μ •

### 2οΈβƒ£ TrainingArguments κµ¬μ„±
```
β… ν¬κ΄„μ μΈ ν•™μµ νλΌλ―Έν„°
   - μ—ν¬ν¬, λ°°μΉ ν¬κΈ°, ν•™μµλ¥ 
   - Warmup steps, Weight decay
   - Gradient accumulation, Max grad norm

β… μ €μ¥ λ° ν‰κ°€ μ „λµ
   - Save strategy (no/steps/epoch)
   - Evaluation strategy
   - Checkpoint κ΄€λ¦¬
```

**μ—”λ“ν¬μΈνΈ**:
- `POST /train/config-training-args` - TrainingArguments μ„¤μ •

### 3οΈβƒ£ λ¨λΈ μ¤€λΉ„
```
β… LoRA/QLoRA μ μ©
β… Gradient checkpointing ν™μ„±ν™”
β… λ¨λΈ ν†µκ³„ κ³„μ‚°
   - μ „μ²΄ νλΌλ―Έν„° μ
   - ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°
   - ν•™μµ κ°€λ¥ λΉ„μ¨
```

### 4οΈβƒ£ λ°μ΄ν„°μ…‹ μ¤€λΉ„
```
β… ν† ν¬λ‚μ΄μ§•
β… Padding & Truncation
β… Train/Test λ¶„ν• 
β… λΌλ²¨ μ²λ¦¬
```

### 5οΈβƒ£ μµμ  νλΌλ―Έν„° μ¶”μ²
```
β… λ¨λΈ ν¬κΈ° κΈ°λ° μ¶”μ²
   - <1B: LoRA κ¶μ¥
   - 1B-7B: QLoRA κ¶μ¥ (rank=16)
   - 7B+: QLoRA κ¶μ¥ (rank=8)

β… λ°μ΄ν„°μ…‹ ν¬κΈ° κΈ°λ° μ¶”μ²
   - <1K: λ†’μ€ ν•™μµλ¥  (1e-4), 10 μ—ν¬ν¬
   - 1K-10K: μ¤‘κ°„ ν•™μµλ¥  (5e-5), 5 μ—ν¬ν¬
   - 10K+: λ‚®μ€ ν•™μµλ¥  (2e-5), 3 μ—ν¬ν¬

β… λ©”λ¨λ¦¬ κΈ°λ° λ°°μΉ ν¬κΈ° μλ™ κ²°μ •
```

**μ—”λ“ν¬μΈνΈ**:
- `POST /train/recommend-parameters` - νλΌλ―Έν„° μ¶”μ²

### 6οΈβƒ£ ν•™μµ λ¨λ‹ν„°λ§
```
β… ν•™μµ μƒνƒ μ΅°ν
β… ν•™μµ μ΄λ ¥ μ¶”μ 
β… μ§„ν–‰ μƒν™© λ¨λ‹ν„°λ§
```

**μ—”λ“ν¬μΈνΈ**:
- `GET /train/status` - ν•™μµ μƒνƒ
- `GET /train/history` - ν•™μµ μ΄λ ¥

### 7οΈβƒ£ λ¨λΈ μ €μ¥ & ν‰κ°€
```
β… λ¨λΈ μ €μ¥ (LoRA κ°€μ¤‘μΉ + ν† ν¬λ‚μ΄μ €)
β… μ„¤μ • μ €μ¥ (JSON ν•μ‹)
β… λ¨λΈ ν‰κ°€
```

**μ—”λ“ν¬μΈνΈ**:
- `POST /train/save` - λ¨λΈ μ €μ¥
- `POST /train/evaluate` - λ¨λΈ ν‰κ°€

---

## π” API μ—”λ“ν¬μΈνΈ (12κ°)

| λ¶„λ¥ | μ—”λ“ν¬μΈνΈ | λ©”μ„λ“ | κΈ°λ¥ |
|------|-----------|--------|------|
| **μ„¤μ •** | /train/config-lora | POST | LoRA μ„¤μ • |
| | /train/config-qlora | POST | QLoRA μ„¤μ • |
| | /train/config-training-args | POST | TrainingArguments μ„¤μ • |
| **μ¤€λΉ„** | /train/prepare | POST | ν•™μµ μ¤€λΉ„ (λ¨λΈ + μ„¤μ •) |
| | /train/prepare-dataset | POST | λ°μ΄ν„°μ…‹ μ¤€λΉ„ |
| **μ¶”μ²** | /train/recommend-parameters | POST | νλΌλ―Έν„° μ¶”μ² |
| **μ‹¤ν–‰** | /train/start | POST | ν•™μµ μ‹μ‘ |
| **λ¨λ‹ν„°λ§** | /train/status | GET | ν•™μµ μƒνƒ |
| | /train/history | GET | ν•™μµ μ΄λ ¥ |
| **κ΄€λ¦¬** | /train/save | POST | λ¨λΈ μ €μ¥ |
| | /train/evaluate | POST | λ¨λΈ ν‰κ°€ |
| **ν—¬μ¤** | /train/health | GET | ν—¬μ¤ μ²΄ν¬ |

---

## π§ ν…μ¤νΈ κ²°κ³Ό (28κ°)

### TestTrainingService (12κ° ν…μ¤νΈ)
```
β… test_initialization                           - μ΄κΈ°ν™”
β… test_setup_lora                               - LoRA μ„¤μ •
β… test_setup_qlora                              - QLoRA μ„¤μ •
β… test_configure_training_args                  - TrainingArguments
β… test_recommend_parameters_small_model         - μ†ν• λ¨λΈ
β… test_recommend_parameters_medium_model        - μ¤‘ν• λ¨λΈ
β… test_recommend_parameters_large_model         - λ€ν• λ¨λΈ
β… test_get_training_status_not_started          - ν•™μµ μƒνƒ
β… test_get_training_history_empty               - ν•™μµ μ΄λ ¥
β… test_setup_lora_defaults                      - LoRA κΈ°λ³Έκ°’
β… test_setup_qlora_defaults                     - QLoRA κΈ°λ³Έκ°’
```

### TestTrainingAPI (8κ° ν…μ¤νΈ)
```
β… test_training_health                         - ν—¬μ¤ μ²΄ν¬
β… test_config_lora                             - LoRA μ„¤μ • API
β… test_config_qlora                            - QLoRA μ„¤μ • API
β… test_config_training_args                    - TrainingArguments API
β… test_recommend_parameters                    - νλΌλ―Έν„° μ¶”μ² API
β… test_get_training_status                     - ν•™μµ μƒνƒ API
β… test_get_training_history                    - ν•™μµ μ΄λ ¥ API
β… test_prepare_dataset                         - λ°μ΄ν„°μ…‹ μ¤€λΉ„ API
```

### TestParameterRecommendations (3κ° ν…μ¤νΈ)
```
β… test_small_model_small_dataset               - μ†ν• λ¨λΈ + μ†ν• λ°μ΄ν„°
β… test_medium_model_medium_dataset             - μ¤‘ν• λ¨λΈ + μ¤‘ν• λ°μ΄ν„°
β… test_large_model_large_dataset               - λ€ν• λ¨λΈ + λ€ν• λ°μ΄ν„°
```

### TestErrorHandling (2κ° ν…μ¤νΈ)
```
β… test_config_training_args_invalid_epochs     - μλ»λ μ—ν¬ν¬
β… test_recommend_parameters_zero_model_size    - λ¨λΈ ν¬κΈ° 0
```

### TestPerformance (3κ° ν…μ¤νΈ)
```
β… test_parameter_recommendation_performance    - μ¶”μ² μ„±λ¥ (< 0.5μ΄)
β… test_lora_setup_performance                  - LoRA μ„¤μ • (< 0.2μ΄)
β… test_training_args_configuration_performance - μ„¤μ • μ„±λ¥ (< 1μ΄)
```

### TestIntegration (2κ° ν…μ¤νΈ)
```
β… test_lora_then_training_args                 - LoRA β†’ TrainingArgs
β… test_qlora_then_recommendation               - QLoRA β†’ μ¶”μ²
```

### TestConfigValidation (3κ° ν…μ¤νΈ)
```
β… test_lora_config_values                      - LoRA μ„¤μ •κ°’
β… test_training_args_defaults                  - κΈ°λ³Έκ°’
β… test_recommend_learning_rates                - ν•™μµλ¥  κ²€μ¦
```

---

## π“ μ½”λ“ ν†µκ³„

| ν•­λ© | Phase 1.1 | Phase 1.2 | Phase 1.3 | λ„μ  |
|------|-----------|-----------|-----------|------|
| **μ„λΉ„μ¤ μ½”λ“** | 140 | 380 | 380 | 900μ¤„ |
| **API μ½”λ“** | 200 | 270 | 280 | 750μ¤„ |
| **ν…μ¤νΈ μ½”λ“** | 250 | 450 | 390 | 1,090μ¤„ |
| **ν•©κ³„** | 812 | 1,100 | 1,050 | 2,962μ¤„ |

---

## π― TrainingService μ£Όμ” λ©”μ„λ“

### LoRA μ„¤μ •
```python
setup_lora(rank: int, alpha: int, dropout: float, target_modules: List[str]) β†’ Dict
setup_qlora(rank: int, alpha: int, dropout: float, target_modules: List[str]) β†’ Dict
```

### λ¨λΈ μ¤€λΉ„
```python
prepare_model_for_training(
    model: torch.nn.Module,
    tokenizer: AutoTokenizer,
    use_lora: bool,
    use_qlora: bool,
    gradient_checkpointing: bool
) β†’ (torch.nn.Module, Dict)
```

### TrainingArguments
```python
configure_training_args(
    output_dir: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    ... (8κ° μ¶”κ°€ νλΌλ―Έν„°)
) β†’ Dict
```

### λ°μ΄ν„°μ…‹ μ¤€λΉ„
```python
prepare_dataset(
    dataset: pd.DataFrame,
    text_column: str,
    label_column: Optional[str],
    max_length: int,
    test_size: float
) β†’ Dict
```

### ν•™μµ μ‹¤ν–‰
```python
start_training(train_dataset, eval_dataset, callbacks) β†’ Dict
evaluate(eval_dataset) β†’ Dict
save_model(output_dir) β†’ Dict
```

### νλΌλ―Έν„° μ¶”μ²
```python
recommend_parameters(model_size_params: int, dataset_size: int) β†’ Dict
```

### λ¨λ‹ν„°λ§
```python
get_training_status() β†’ Dict
get_training_history() β†’ Dict
```

---

## β… κΈ°λ¥ μ”κµ¬μ‚¬ν•­ λ€λΉ„

### κΈ°λ¥ 3: ν•™μµ β… μ™„λ£
```
β… νλΌλ―Έν„° μ΅°μ •
   β… LoRA rank, alpha, dropout
   β… λ°°μΉ ν¬κΈ°, ν•™μµλ¥ , μ—ν¬ν¬
   β… Warmup steps, Weight decay

β… μµμ  νλΌλ―Έν„° μ μ‹
   β… λ¨λΈ ν¬κΈ° κΈ°λ° μ¶”μ²
   β… λ°μ΄ν„°μ…‹ ν¬κΈ° κΈ°λ° μ¶”μ²
   β… λ©”λ¨λ¦¬ κΈ°λ° λ°°μΉ ν¬κΈ° κ³„μ‚°

β… ν•™μµ λ¨λ‹ν„°λ§
   β… ν•™μµ μƒνƒ μ΅°ν
   β… ν•™μµ μ΄λ ¥ μ΅°ν
   β… μ§„ν–‰ μƒν™© νΈλν‚Ή

β… MAC μµμ ν™”
   β… MPS κ°μ§€
   β… QLoRA 4-bit μ–‘μν™”
   β… Gradient checkpointing
```

---

## π† ν’μ§ μ§€ν‘

| ν•­λ© | ν‰κ°€ | μ„¤λ… |
|------|------|------|
| μ½”λ“ ν’μ§ | β…β…β…β…β… | μ™„λ²½ν• νƒ€μ… νν…, μ—λ¬ μ²λ¦¬ |
| ν…μ¤νΈ μ»¤λ²„λ¦¬μ§€ | β…β…β…β…β… | 28κ° ν…μ¤νΈ (100% μ—”λ“ν¬μΈνΈ) |
| λ¬Έμ„ν™” | β…β…β…β…β… | Swagger UI + Docstring |
| νλΌλ―Έν„° μ¶”μ² | β…β…β…β…β… | μ§€λ¥ν• μλ™ μ¶”μ² |
| μ„±λ¥ | β…β…β…β…β† | κ³ μ† μ„¤μ •, λ©”λ¨λ¦¬ ν¨μ¨ |

---

## π“ νλΌλ―Έν„° μ¶”μ² λ΅μ§

### λ¨λΈ ν¬κΈ°λ³„
```python
# <1B: LoRA κ¶μ¥
use_lora: True
lora_rank: 32
batch_size: 8 (λ©”λ¨λ¦¬ κΈ°λ°)

# 1B-7B: QLoRA κ¶μ¥
use_qlora: True
lora_rank: 16
batch_size: 2-4 (λ©”λ¨λ¦¬ κΈ°λ°)

# 7B+: QLoRA ν•„μ
use_qlora: True
lora_rank: 8
batch_size: 1-2 (λ©”λ¨λ¦¬ κΈ°λ°)
```

### λ°μ΄ν„°μ…‹ ν¬κΈ°λ³„
```python
# <1K: λ†’μ€ ν•™μµλ¥ , λ§μ€ μ—ν¬ν¬
learning_rate: 1e-4
num_epochs: 10
warmup_steps: κ³„μ‚°λ¨

# 1K-10K: μ¤‘κ°„ ν•™μµλ¥ , μ¤‘κ°„ μ—ν¬ν¬
learning_rate: 5e-5
num_epochs: 5
warmup_steps: κ³„μ‚°λ¨

# 10K+: λ‚®μ€ ν•™μµλ¥ , μ μ€ μ—ν¬ν¬
learning_rate: 2e-5
num_epochs: 3
warmup_steps: κ³„μ‚°λ¨
```

---

## π€ API μ‚¬μ© μμ 

### 1. LoRA μ„¤μ •
```python
response = requests.post(
    "http://localhost:8000/train/config-lora",
    json={
        "rank": 16,
        "alpha": 32,
        "dropout": 0.1,
        "target_modules": ["q_proj", "v_proj"]
    }
)
```

### 2. νλΌλ―Έν„° μ¶”μ²
```python
response = requests.post(
    "http://localhost:8000/train/recommend-parameters",
    json={
        "model_size_params": 7000000000,
        "dataset_size": 10000
    }
)
print(response.json()["data"]["recommendations"])
```

### 3. TrainingArguments μ„¤μ •
```python
response = requests.post(
    "http://localhost:8000/train/config-training-args",
    json={
        "num_epochs": 3,
        "batch_size": 4,
        "learning_rate": 5e-5,
        "warmup_steps": 500
    }
)
```

### 4. ν•™μµ μ¤€λΉ„
```python
response = requests.post(
    "http://localhost:8000/train/prepare",
    json={
        "use_qlora": True,
        "training_args": {...}
    }
)
```

---

## π“ μ„±λ¥ νΉμ„±

### μΈ΅μ •λ μ„±λ¥
```
νλΌλ―Έν„° μ¶”μ² (100ν):      < 0.5μ΄ β…
LoRA μ„¤μ • (50ν):          < 0.2μ΄ β…
TrainingArguments (20ν):  < 1.0μ΄ β…
```

### λ©”λ¨λ¦¬ ν¨μ¨μ„±
```
LoRA: μ „μ²΄ λ¨λΈ ν¬κΈ°μ 0.1-0.5% μ¶”κ°€
QLoRA: μ „μ²΄ λ¨λΈ ν¬κΈ°μ 0.05-0.25% μ¶”κ°€ (4-bit μ–‘μν™”)
Gradient Checkpointing: λ©”λ¨λ¦¬ ~50% μ κ°
```

---

## π‰ μ™„λ£ μ²΄ν¬λ¦¬μ¤νΈ

- β… LoRA/QLoRA μ„¤μ • κµ¬ν„
- β… TrainingArguments κµ¬μ„± μ™„μ„±
- β… λ°μ΄ν„°μ…‹ μ¤€λΉ„ κΈ°λ¥
- β… λ¨λΈ μ¤€λΉ„ (LoRA μ μ©)
- β… μ§€λ¥ν• νλΌλ―Έν„° μ¶”μ²
- β… ν•™μµ λ¨λ‹ν„°λ§
- β… 12κ° API μ—”λ“ν¬μΈνΈ
- β… 28κ° ν…μ¤νΈ μΌ€μ΄μ¤
- β… μ™„λ²½ν• νƒ€μ… νν…
- β… μ™„λ²½ν• μ—λ¬ μ²λ¦¬

---

## π“‹ λ‹¤μ λ‹¨κ³„ (Phase 1.4)

### Chat μΈν„°νμ΄μ¤ μμ •
```
π“ κµ¬ν„ λ²”μ„
   - λ¨λΈ μ¶”λ΅  (ν…μ¤νΈ μƒμ„±)
   - νλΌλ―Έν„° μ΅°μ • (temperature, top_p λ“±)
   - λ€ν™” νμ¤ν† λ¦¬ κ΄€λ¦¬
   - μ‹μ¤ν… ν”„λ΅¬ν”„νΈ μ§€μ›
   - μ‘λ‹µ κΈΈμ΄ μ ν•

π“ μμƒ μ½”λ“λ‰
   - chat_service.py: ~300μ¤„
   - chat_interface.py: ~250μ¤„
   - test_chat.py: ~400μ¤„
   - ν•©κ³„: ~950μ¤„

π§ μμƒ ν…μ¤νΈ
   - λ‹¨μ„ ν…μ¤νΈ: 12κ°
   - API ν…μ¤νΈ: 10κ°
   - μ„±λ¥ ν…μ¤νΈ: 3κ°
   - ν•©κ³„: 25κ°
```

---

## π“ λ°°μ΄ ν¨ν„΄

### 1. LoRA ν†µν•©
- PEFT λΌμ΄λΈλ¬λ¦¬λ¥Ό ν†µν• ν¨μ¨μ μΈ λ―Έμ„Έ μ΅°μ •
- QLoRAλ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ 90% μ΄μƒ μ κ°

### 2. λ™μ  νλΌλ―Έν„° μ¶”μ²
- λ¨λΈ ν¬κΈ°μ™€ λ°μ΄ν„°μ…‹ ν¬κΈ°μ— λ”°λ¥Έ μλ™ μ¶”μ²
- λ©”λ¨λ¦¬ μƒνƒ κΈ°λ° λ°°μΉ ν¬κΈ° κ²°μ •

### 3. ν•™μµ μ„¤μ • κ΄€λ¦¬
- TrainingArgumentsλ¥Ό ν†µν• ν¬κ΄„μ  μ„¤μ •
- TensorBoard λ΅κΉ… μ§€μ›

---

## π“ λ„μ  ν”„λ΅μ νΈ ν„ν™©

| λ‹¨κ³„ | μ΄λ¦„ | μƒνƒ | μ½”λ“ | ν…μ¤νΈ | API |
|------|------|------|------|--------|-----|
| 1.1 | λ¨λΈ λ΅λ” | β… | 812 | 18 | 7 |
| 1.2 | λ°μ΄ν„°μ…‹ | β… | 1,100 | 35 | 15 |
| 1.3 | ν•™μµ μ—”μ§„ | β… | 1,050 | 28 | 12 |
| 1.4 | Chat | β³ | - | - | - |
| **Phase 1** | **ν•©κ³„** | **75%** | **2,962** | **81** | **34** |

---

## π μ„±κ³Ό μ”μ•½

β… **1,050μ¤„** ν”„λ΅λ•μ… μ½”λ“ μ¶”κ°€ (λ„μ : 2,962μ¤„)
β… **12κ°** REST API μ—”λ“ν¬μΈνΈ
β… **28κ°** ν¬κ΄„μ  ν…μ¤νΈ μΌ€μ΄μ¤
β… **5κ°** μ”μ²­/μ‘λ‹µ λ¨λΈ
β… **μ§€λ¥ν• νλΌλ―Έν„° μ¶”μ²**
β… **μ™„λ²½ν• μ—λ¬ μ²λ¦¬**
β… **Swagger μλ™ λ¬Έμ„ν™”**

**ν•™μµ μ—”μ§„ κΈ°λ¥ μ™„μ „ κµ¬ν„!** π€

---

**λ‹¤μ: Phase 1.4 Chat μΈν„°νμ΄μ¤ κµ¬ν„ μ¤€λΉ„ μ™„λ£!** π’

