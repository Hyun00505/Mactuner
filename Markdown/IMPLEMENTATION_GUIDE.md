# MacTuner êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •](#í”„ë¡œì íŠ¸-ì´ˆê¸°-ì„¤ì •)
2. [ë°±ì—”ë“œ êµ¬í˜„](#ë°±ì—”ë“œ-êµ¬í˜„)
3. [í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„](#í”„ë¡ íŠ¸ì—”ë“œ-êµ¬í˜„)
4. [í†µí•© ë° í…ŒìŠ¤íŠ¸](#í†µí•©-ë°-í…ŒìŠ¤íŠ¸)
5. [ë°°í¬](#ë°°í¬)

---

## í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •

### 1.1 ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
MacTuner/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ db.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â”œâ”€â”€ dataset_tools.py
â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â”œâ”€â”€ chat_interface.py
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”‚   â””â”€â”€ export_gguf.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_service.py
â”‚   â”‚   â”œâ”€â”€ training_service.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ quantization_service.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ validators.py
â”‚   â”‚   â””â”€â”€ mac_optimization.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ backend/
â”‚   â””â”€â”€ frontend/
â”œâ”€â”€ docs/
â”œâ”€â”€ SPECIFICATION.md
â”œâ”€â”€ IMPLEMENTATION_GUIDE.md
â””â”€â”€ README.md
```

### 1.2 ë°±ì—”ë“œ ì´ˆê¸° ì„¤ì •

```bash
# Python í™˜ê²½ ì„¤ì •
python -m venv venv
source venv/bin/activate

# requirements.txt ìƒì„±
pip install -r backend/requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
```

### 1.3 í”„ë¡ íŠ¸ì—”ë“œ ì´ˆê¸° ì„¤ì •

```bash
# Node.js í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
cd frontend
npm create vite@latest . -- --template react-ts
npm install

# UI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
npm install @radix-ui/react-* lucide-react recharts zustand @tanstack/react-query axios
```

---

## ë°±ì—”ë“œ êµ¬í˜„

### 2.1 MAC ìµœì í™” ìœ í‹¸ë¦¬í‹° (mac_optimization.py)

```python
# backend/utils/mac_optimization.py
import torch
import psutil
import os

class MACOptimizer:
    """MAC (Apple Silicon) ìµœì í™” ê´€ë ¨ ìœ í‹¸ë¦¬í‹°"""

    @staticmethod
    def get_device():
        """MAC MPS ë˜ëŠ” CPU ì„ íƒ"""
        if torch.backends.mps.is_available():
            return torch.device("mps")
        elif torch.cuda.is_available():
            return torch.device("cuda")
        return torch.device("cpu")

    @staticmethod
    def get_optimal_batch_size(model_params: int) -> int:
        """ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸°"""
        # ê°€ìš© ë©”ëª¨ë¦¬ í™•ì¸
        memory_gb = psutil.virtual_memory().available / (1024**3)

        if model_params > 7e9:  # 7B+
            return max(1, int(memory_gb / 8))
        elif model_params > 1e9:  # 1B-7B
            return max(4, int(memory_gb / 4))
        else:  # <1B
            return max(8, int(memory_gb / 2))

    @staticmethod
    def get_memory_stats():
        """ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„"""
        return {
            "total_gb": psutil.virtual_memory().total / (1024**3),
            "available_gb": psutil.virtual_memory().available / (1024**3),
            "percent": psutil.virtual_memory().percent,
            "device": str(MACOptimizer.get_device())
        }
```

### 2.2 ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ (model_service.py)

```python
# backend/services/model_service.py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os
from utils.mac_optimization import MACOptimizer

class ModelService:
    def __init__(self):
        self.device = MACOptimizer.get_device()
        self.cache_dir = os.path.expanduser("~/.cache/huggingface/hub")

    def load_from_hub(self, model_id: str, token: str = None):
        """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                token=token,
                cache_dir=self.cache_dir
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                token=token,
                device_map="auto",
                torch_dtype=torch.float16 if self.device.type == "mps" else "auto",
                cache_dir=self.cache_dir
            )
            return model, tokenizer, self._extract_metadata(model)
        except Exception as e:
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def load_local(self, path: str):
        """ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}")

        try:
            tokenizer = AutoTokenizer.from_pretrained(path)
            model = AutoModelForCausalLM.from_pretrained(
                path,
                device_map="auto",
                torch_dtype=torch.float16
            )
            return model, tokenizer, self._extract_metadata(model)
        except Exception as e:
            raise RuntimeError(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def _extract_metadata(self, model):
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        return {
            "model_type": model.config.model_type,
            "hidden_size": model.config.hidden_size,
            "num_layers": model.config.num_hidden_layers if hasattr(model.config, 'num_hidden_layers') else 0,
            "vocab_size": model.config.vocab_size,
            "num_parameters": sum(p.numel() for p in model.parameters()),
            "device": str(self.device)
        }
```

### 2.3 í•™ìŠµ ì„œë¹„ìŠ¤ (training_service.py)

```python
# backend/services/training_service.py
from peft import get_peft_model, LoraConfig, TaskType
from transformers import Trainer, TrainingArguments
import torch
from utils.mac_optimization import MACOptimizer

class TrainingService:
    def __init__(self):
        self.device = MACOptimizer.get_device()

    def get_lora_config(self, model_size_params: int):
        """ëª¨ë¸ í¬ê¸°ì— ë§ëŠ” LoRA ì„¤ì •"""
        if model_size_params < 1e9:
            rank, alpha = 16, 32
        elif model_size_params < 7e9:
            rank, alpha = 8, 16
        else:
            rank, alpha = 4, 8

        return LoraConfig(
            r=rank,
            lora_alpha=alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

    def prepare_model(self, model, config: dict):
        """ëª¨ë¸ í•™ìŠµ ì¤€ë¹„"""
        if config.get("method") == "lora":
            peft_config = self.get_lora_config(model.num_parameters())
            model = get_peft_model(model, peft_config)

        return model

    def get_training_args(self, config: dict):
        """íŠ¸ë ˆì´ë‹ ì¸ì ì„¤ì •"""
        return TrainingArguments(
            output_dir=config.get("output_dir", "./results"),
            num_train_epochs=config.get("epochs", 3),
            per_device_train_batch_size=config.get("batch_size", 4),
            per_device_eval_batch_size=config.get("batch_size", 4) * 2,
            learning_rate=config.get("learning_rate", 5e-5),
            warmup_steps=config.get("warmup_steps", 500),
            logging_steps=10,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            gradient_checkpointing=True,
            fp16=self.device.type in ["cuda", "mps"],
            logging_dir="./logs",
        )
```

### 2.4 RAG ì„œë¹„ìŠ¤ (rag_service.py)

```python
# backend/services/rag_service.py
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from PyPDF2 import PdfReader

class RAGService:
    def __init__(self):
        self.embedding_model = None
        self.index = None
        self.chunks = []

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        step = chunk_size - overlap
        for i in range(0, len(text), step):
            chunks.append(text[i:i + chunk_size])
        return chunks

    def build_vector_store(self, chunks: list[str], model_name: str):
        """ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        self.embedding_model = SentenceTransformer(model_name)
        embeddings = self.embedding_model.encode(chunks, convert_to_numpy=True)

        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype(np.float32))
        self.chunks = chunks

    def retrieve(self, query: str, top_k: int = 5):
        """ê´€ë ¨ ì²­í¬ ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)
        distances, indices = self.index.search(query_embedding.astype(np.float32), top_k)

        scores = 1 / (1 + distances[0])
        results = [(self.chunks[i], float(scores[j])) for j, i in enumerate(indices[0])]
        return results
```

### 2.5 API ë¼ìš°í„° êµ¬í˜„

#### 2.5.1 ëª¨ë¸ ë¡œë” API

```python
# backend/api/model_loader.py (ìˆ˜ì •)
from fastapi import APIRouter, HTTPException, File, UploadFile
from pydantic import BaseModel
from services.model_service import ModelService

router = APIRouter(tags=["model"])
model_service = ModelService()
_MODEL_CACHE = {}

class ModelDownloadRequest(BaseModel):
    model_id: str
    access_token: str = None

class ModelDownloadResponse(BaseModel):
    status: str
    metadata: dict

@router.post("/download", response_model=ModelDownloadResponse)
async def download_model(payload: ModelDownloadRequest):
    """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        model, tokenizer, metadata = model_service.load_from_hub(
            payload.model_id,
            payload.access_token
        )
        _MODEL_CACHE.update({
            "model": model,
            "tokenizer": tokenizer,
            "metadata": metadata
        })
        return ModelDownloadResponse(status="success", metadata=metadata)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/upload")
async def upload_model(file: UploadFile = File(...)):
    """ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ ì €ì¥ ë° ê²€ì¦
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        model, tokenizer, metadata = model_service.load_local(tmp_path)
        _MODEL_CACHE.update({
            "model": model,
            "tokenizer": tokenizer,
            "metadata": metadata
        })
        return {"status": "success", "metadata": metadata}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
```

#### 2.5.2 ë°ì´í„°ì…‹ ë„êµ¬ API

```python
# backend/api/dataset_tools.py (êµ¬í˜„)
from fastapi import APIRouter, HTTPException, UploadFile, File
import pandas as pd
import io
import json

router = APIRouter(tags=["dataset"])

@router.post("/upload")
async def upload_dataset(file: UploadFile = File(...)):
    """ë°ì´í„°ì…‹ ì—…ë¡œë“œ"""
    try:
        content = await file.read()

        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(content))
        elif file.filename.endswith('.json'):
            df = pd.read_json(io.BytesIO(content))
        else:
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")

        stats = {
            "rows": len(df),
            "columns": df.columns.tolist(),
            "memory_mb": df.memory_usage(deep=True).sum() / 1024**2,
            "preview": df.head(5).to_dict()
        }

        return {"status": "success", "stats": stats}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/analyze")
async def analyze_dataset(file: UploadFile = File(...)):
    """ë°ì´í„°ì…‹ ë¶„ì„"""
    try:
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content))

        analysis = {
            "describe": df.describe().to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "duplicates": df.duplicated().sum(),
        }

        return {"status": "success", "analysis": analysis}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
```

#### 2.5.3 í•™ìŠµ API

```python
# backend/api/training.py (êµ¬í˜„)
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from services.training_service import TrainingService
from transformers import Trainer
import asyncio

router = APIRouter(tags=["training"])
training_service = TrainingService()
_TRAINING_STATE = {}

class TrainingConfig(BaseModel):
    method: str  # "full", "lora", "qlora"
    epochs: int = 3
    batch_size: int = 4
    learning_rate: float = 5e-5
    warmup_steps: int = 500
    output_dir: str = "./results"

@router.post("/start")
async def start_training(config: TrainingConfig):
    """í•™ìŠµ ì‹œì‘"""
    try:
        # ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì¤€ë¹„ ë¡œì§
        # (ì„¸ë¶€ êµ¬í˜„ì€ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‘ì„±)
        return {"status": "started", "training_id": "train_001"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/status/{training_id}")
async def get_training_status(training_id: str):
    """í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
    return _TRAINING_STATE.get(training_id, {"status": "not_found"})
```

---

## í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„

### 3.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
frontend/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”‚   â””â”€â”€ Layout.tsx
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”‚   â”œâ”€â”€ ModelManagement.tsx
â”‚   â”‚   â”œâ”€â”€ DataPreparation.tsx
â”‚   â”‚   â”œâ”€â”€ Training.tsx
â”‚   â”‚   â”œâ”€â”€ Chat.tsx
â”‚   â”‚   â”œâ”€â”€ RAG.tsx
â”‚   â”‚   â””â”€â”€ GGUFExport.tsx
â”‚   â””â”€â”€ forms/
â”‚       â”œâ”€â”€ ModelDownloadForm.tsx
â”‚       â”œâ”€â”€ TrainingConfigForm.tsx
â”‚       â””â”€â”€ RAGSetupForm.tsx
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ index.tsx
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api.ts
â”‚   â””â”€â”€ websocket.ts
â”œâ”€â”€ store/
â”‚   â””â”€â”€ appStore.ts
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useTrainingMonitor.ts
â”œâ”€â”€ types/
â”‚   â””â”€â”€ index.ts
â””â”€â”€ App.tsx
```

### 3.2 íƒ€ì… ì •ì˜

```typescript
// frontend/src/types/index.ts
export interface Model {
  id: string;
  name: string;
  source: "hub" | "local";
  size: number;
  parameters: number;
  metadata: Record<string, any>;
}

export interface Dataset {
  id: string;
  name: string;
  rows: number;
  columns: string[];
  size: number;
}

export interface TrainingConfig {
  method: "full" | "lora" | "qlora";
  epochs: number;
  batch_size: number;
  learning_rate: number;
  warmup_steps: number;
  output_dir: string;
}

export interface TrainingState {
  status: "idle" | "running" | "completed" | "failed";
  epoch: number;
  step: number;
  loss: number;
  eval_loss?: number;
  progress: number;
}

export interface ChatMessage {
  role: "user" | "assistant";
  content: string;
  timestamp: Date;
}
```

### 3.3 ìƒíƒœ ê´€ë¦¬

```typescript
// frontend/src/store/appStore.ts
import { create } from "zustand";
import { Model, Dataset, TrainingState } from "../types";

interface AppState {
  // Models
  models: Model[];
  selectedModel: Model | null;
  setSelectedModel: (model: Model) => void;

  // Datasets
  datasets: Dataset[];
  selectedDataset: Dataset | null;
  setSelectedDataset: (dataset: Dataset) => void;

  // Training
  trainingState: TrainingState;
  updateTrainingState: (state: Partial<TrainingState>) => void;

  // UI
  sidebarOpen: boolean;
  toggleSidebar: () => void;
}

export const useAppStore = create<AppState>((set) => ({
  models: [],
  selectedModel: null,
  setSelectedModel: (model) => set({ selectedModel: model }),

  datasets: [],
  selectedDataset: null,
  setSelectedDataset: (dataset) => set({ selectedDataset: dataset }),

  trainingState: {
    status: "idle",
    epoch: 0,
    step: 0,
    loss: 0,
    progress: 0,
  },
  updateTrainingState: (state) =>
    set((prev) => ({
      trainingState: { ...prev.trainingState, ...state },
    })),

  sidebarOpen: true,
  toggleSidebar: () => set((prev) => ({ sidebarOpen: !prev.sidebarOpen })),
}));
```

### 3.4 API ì„œë¹„ìŠ¤

```typescript
// frontend/src/services/api.ts
import axios from "axios";

const API_BASE_URL = process.env.REACT_APP_API_URL || "http://localhost:8000";

const api = axios.create({
  baseURL: API_BASE_URL,
});

export const modelAPI = {
  downloadFromHub: (modelId: string, token?: string) => api.post("/model/download", { model_id: modelId, access_token: token }),

  uploadLocal: (file: File) => {
    const formData = new FormData();
    formData.append("file", file);
    return api.post("/model/upload", formData);
  },
};

export const datasetAPI = {
  upload: (file: File) => {
    const formData = new FormData();
    formData.append("file", file);
    return api.post("/dataset/upload", formData);
  },

  analyze: (file: File) => {
    const formData = new FormData();
    formData.append("file", file);
    return api.post("/dataset/analyze", formData);
  },
};

export const trainingAPI = {
  start: (config: any) => api.post("/train/start", config),

  getStatus: (trainingId: string) => api.get(`/train/status/${trainingId}`),
};

export const chatAPI = {
  chat: (message: string, modelId: string, config?: any) => api.post("/chat/generate", { message, model_id: modelId, ...config }),
};

export const ragAPI = {
  uploadDocument: (file: File) => {
    const formData = new FormData();
    formData.append("file", file);
    return api.post("/rag/upload", formData);
  },

  buildIndex: (config: any) => api.post("/rag/build", config),

  search: (query: string, topK: number = 5) => api.post("/rag/search", { query, top_k: topK }),
};

export const ggufAPI = {
  convertModel: (modelPath: string, quantizationType: string) => api.post("/export/gguf", { model_path: modelPath, quantization_type: quantizationType }),
};
```

### 3.5 ì£¼ìš” ì»´í¬ë„ŒíŠ¸

#### 3.5.1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¼

```typescript
// frontend/src/components/forms/ModelDownloadForm.tsx
import React, { useState } from "react";
import { modelAPI } from "../../services/api";

export const ModelDownloadForm: React.FC = () => {
  const [modelId, setModelId] = useState("");
  const [token, setToken] = useState("");
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const handleDownload = async () => {
    setLoading(true);
    setError(null);

    try {
      const response = await modelAPI.downloadFromHub(modelId, token);
      console.log("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ:", response.data);
      // ì„±ê³µ ì²˜ë¦¬
    } catch (err: any) {
      setError(err.response?.data?.detail || "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-6 bg-white rounded-lg shadow">
      <h2 className="text-2xl font-bold mb-4">ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</h2>

      <div className="space-y-4">
        <div>
          <label className="block text-sm font-medium mb-2">ëª¨ë¸ ID</label>
          <input type="text" value={modelId} onChange={(e) => setModelId(e.target.value)} placeholder="ì˜ˆ: meta-llama/Llama-2-7b" className="w-full px-4 py-2 border rounded-lg" />
        </div>

        <div>
          <label className="block text-sm font-medium mb-2">Hugging Face Token</label>
          <input type="password" value={token} onChange={(e) => setToken(e.target.value)} placeholder="í† í° ì…ë ¥" className="w-full px-4 py-2 border rounded-lg" />
        </div>

        {error && <div className="p-4 bg-red-50 text-red-700 rounded-lg">{error}</div>}

        <button onClick={handleDownload} disabled={!modelId || loading} className="w-full px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:opacity-50">
          {loading ? "ë‹¤ìš´ë¡œë“œ ì¤‘..." : "ë‹¤ìš´ë¡œë“œ"}
        </button>
      </div>
    </div>
  );
};
```

#### 3.5.2 í•™ìŠµ ëª¨ë‹ˆí„° í›…

```typescript
// frontend/src/hooks/useTrainingMonitor.ts
import { useEffect, useCallback } from "react";
import { useAppStore } from "../store/appStore";
import { trainingAPI } from "../services/api";

export const useTrainingMonitor = (trainingId: string | null) => {
  const updateTrainingState = useAppStore((state) => state.updateTrainingState);

  const pollTrainingStatus = useCallback(async () => {
    if (!trainingId) return;

    try {
      const response = await trainingAPI.getStatus(trainingId);
      updateTrainingState(response.data);
    } catch (error) {
      console.error("í•™ìŠµ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨:", error);
    }
  }, [trainingId, updateTrainingState]);

  useEffect(() => {
    const interval = setInterval(pollTrainingStatus, 2000);
    return () => clearInterval(interval);
  }, [pollTrainingStatus]);
};
```

---

## í†µí•© ë° í…ŒìŠ¤íŠ¸

### 4.1 ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸

```python
# tests/backend/test_model_loader.py
import pytest
from backend.services.model_service import ModelService

@pytest.fixture
def model_service():
    return ModelService()

def test_load_from_hub(model_service):
    """Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    model, tokenizer, metadata = model_service.load_from_hub(
        "gpt2"
    )
    assert model is not None
    assert tokenizer is not None
    assert "num_parameters" in metadata
```

### 4.2 API í…ŒìŠ¤íŠ¸

```python
# tests/backend/test_api.py
import pytest
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)

def test_model_download_endpoint():
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
    response = client.post(
        "/model/download",
        json={"model_id": "gpt2"}
    )
    assert response.status_code == 200
    assert response.json()["status"] == "success"
```

### 4.3 í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸

```typescript
// tests/frontend/ModelDownloadForm.test.tsx
import { render, screen, fireEvent } from "@testing-library/react";
import { ModelDownloadForm } from "../../src/components/forms/ModelDownloadForm";

test("renders download form", () => {
  render(<ModelDownloadForm />);
  expect(screen.getByText("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")).toBeInTheDocument();
});

test("handles model download", async () => {
  render(<ModelDownloadForm />);

  const input = screen.getByPlaceholderText(/ëª¨ë¸ ID/i);
  fireEvent.change(input, { target: { value: "gpt2" } });

  const button = screen.getByText("ë‹¤ìš´ë¡œë“œ");
  fireEvent.click(button);

  // í…ŒìŠ¤íŠ¸ ë¡œì§
});
```

---

## ë°°í¬

### 5.1 Docker ë°°í¬

```dockerfile
# docker/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ë°±ì—”ë“œ ì˜ì¡´ì„±
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Node ì„¤ì¹˜
RUN apt-get update && apt-get install -y nodejs npm
RUN rm -rf /var/lib/apt/lists/*

# í”„ë¡ íŠ¸ì—”ë“œ ì˜ì¡´ì„±
COPY frontend/package*.json ./frontend/
WORKDIR /app/frontend
RUN npm ci --omit=dev

# í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ
COPY frontend .
RUN npm run build

WORKDIR /app

# ì½”ë“œ ë³µì‚¬
COPY backend .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì•± ì‹¤í–‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.2 Docker Compose

```yaml
# docker/docker-compose.yml
version: "3.8"

services:
  mactuner:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./data:/app/data
    environment:
      - PYTHONUNBUFFERED=1
```

### 5.3 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] ì •ì  íŒŒì¼ ìµœì í™”
- [ ] CORS ì„¤ì • í™•ì¸
- [ ] ë¡œê¹… ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì—°ê²°
- [ ] ë³´ì•ˆ í—¤ë” ì„¤ì •
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

---

## ê²°ë¡ 

ì´ êµ¬í˜„ ê°€ì´ë“œëŠ” MacTuner ê°œë°œì˜ ê¸°ë³¸ í‹€ì„ ì œì‹œí•©ë‹ˆë‹¤. ê° ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ë©´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ê¸°ëŠ¥ì„ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.
