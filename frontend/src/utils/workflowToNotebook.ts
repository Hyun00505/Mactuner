/**
 * ì›Œí¬í”Œë¡œìš°ë¥¼ Jupyter Notebook (.ipynb) í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
 */

import { Node, Connection } from "../types/editor";

interface NotebookCell {
  cell_type: "code" | "markdown";
  source: string[];
  metadata?: Record<string, any>;
  execution_count?: number | null;
  outputs?: any[];
}

interface Notebook {
  cells: NotebookCell[];
  metadata: {
    kernelspec: {
      display_name: string;
      language: string;
      name: string;
    };
    language_info: {
      name: string;
      version: string;
    };
  };
  nbformat: number;
  nbformat_minor: number;
}

/**
 * ë…¸ë“œ íƒ€ì…ë³„ Python ì½”ë“œ ìƒì„±
 */
function generateNodeCode(node: Node, nodeOutputs: Record<string, any>): string {
  const nodeData = node.data as any;
  let code = "";

  switch (node.type) {
    case "hf-token": {
      const token = nodeData.token || "";
      code = `# HuggingFace í† í° ì„¤ì •
import os
os.environ["HF_TOKEN"] = "${token}"

# ë˜ëŠ” ì§ì ‘ ì‚¬ìš©
from huggingface_hub import login
login(token="${token}")
print("âœ… HuggingFace í† í°ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")`;
      break;
    }

    case "hf-model-downloader": {
      const modelId = nodeData.modelId || "";
      code = `# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "${modelId}"
print(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_id}")

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_id}")`;
      break;
    }

    case "hf-dataset-downloader": {
      const datasetId = nodeData.datasetId || "";
      code = `# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
from datasets import load_dataset

dataset_id = "${datasetId}"
print(f"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘: {dataset_id}")

dataset = load_dataset(dataset_id)
print(f"âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dataset_id}")
print(f"   ì»¬ëŸ¼: {list(dataset['train'].column_names) if 'train' in dataset else list(dataset[list(dataset.keys())[0]].column_names) if dataset else [])}")`;
      break;
    }

    case "dataset-preprocessor": {
      const format = nodeData.format || "instruction";
      const template = nodeData.template || "";
      const maxLength = nodeData.maxLength || 512;
      const inputColumns = nodeData.inputColumns || [];
      const outputColumns = nodeData.outputColumns || [];
      const outputSeparator = nodeData.outputSeparator || " ";

      code = `# ë°ì´í„° ì „ì²˜ë¦¬
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ì„¤ì • (ëª¨ë¸ê³¼ ë™ì¼í•œ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
# tokenizer = AutoTokenizer.from_pretrained(model_id)

def preprocess_dataset(examples):
    """
    ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ í•¨ìˆ˜
    """
    texts = []
    
    if "${format}" == "instruction":
        # Instruction í¬ë§·: input + output
        input_cols = ${JSON.stringify(inputColumns)}
        output_cols = ${JSON.stringify(outputColumns)}
        separator = "${outputSeparator}"
        
        for i in range(len(examples[input_cols[0]])):
            input_text = separator.join([str(examples[col][i]) for col in input_cols if col in examples])
            output_text = separator.join([str(examples[col][i]) for col in output_cols if col in examples])
            
            template_str = """${template || "### Instruction:\\n{input}\\n\\n### Response:\\n{output}"}"""
            text = template_str.replace("{input}", input_text).replace("{output}", output_text)
            texts.append(text)
    
    elif "${format}" == "chat":
        # Chat í¬ë§·
        user_col = "${nodeData.userColumn || ""}"
        assistant_col = "${nodeData.assistantColumn || ""}"
        system_col = "${nodeData.systemColumn || ""}"
        
        for i in range(len(examples[user_col])):
            messages = []
            if system_col and system_col in examples:
                messages.append({"role": "system", "content": str(examples[system_col][i])})
            messages.append({"role": "user", "content": str(examples[user_col][i])})
            messages.append({"role": "assistant", "content": str(examples[assistant_col][i])})
            
            # Chat í…œí”Œë¦¿ ì ìš©
            text = tokenizer.apply_chat_template(messages, tokenize=False)
            texts.append(text)
    
    else:
        # Causal LM í¬ë§·
        text_col = "${nodeData.textColumn || ""}"
        for i in range(len(examples[text_col])):
            texts.append(str(examples[text_col][i]))
    
    # í† í°í™”
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=${maxLength},
        return_tensors="pt"
    )
    
    return encodings

# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš©
processed_dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=dataset['train'].column_names)
print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")`;
      break;
    }

    case "dataset-splitter": {
      const trainRatio = nodeData.trainRatio || 80;
      const valRatio = nodeData.valRatio || 10;
      const testRatio = nodeData.testRatio || 10;
      const shuffle = nodeData.shuffle !== false;

      code = `# ë°ì´í„°ì…‹ ë¶„í• 
train_ratio = ${trainRatio / 100}
val_ratio = ${valRatio / 100}
test_ratio = ${testRatio / 100}

# ë°ì´í„°ì…‹ ë¶„í• 
if 'train' in processed_dataset:
    split_dataset = processed_dataset['train'].train_test_split(
        test_size=val_ratio + test_ratio,
        shuffle=${shuffle}
    )
    train_dataset = split_dataset['train']
    
    if test_ratio > 0:
        val_test_split = split_dataset['test'].train_test_split(
            test_size=test_ratio / (val_ratio + test_ratio),
            shuffle=${shuffle}
        )
        val_dataset = val_test_split['train']
        test_dataset = val_test_split['test']
    else:
        val_dataset = split_dataset['test']
        test_dataset = None
else:
    # ì „ì²´ ë°ì´í„°ì…‹ì„ ë¶„í• 
    split_dataset = processed_dataset.train_test_split(
        test_size=val_ratio + test_ratio,
        shuffle=${shuffle}
    )
    train_dataset = split_dataset['train']
    
    if test_ratio > 0:
        val_test_split = split_dataset['test'].train_test_split(
            test_size=test_ratio / (val_ratio + test_ratio),
            shuffle=${shuffle}
        )
        val_dataset = val_test_split['train']
        test_dataset = val_test_split['test']
    else:
        val_dataset = split_dataset['test']
        test_dataset = None

print(f"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ")
print(f"   í•™ìŠµ: {len(train_dataset)}ê°œ")
print(f"   ê²€ì¦: {len(val_dataset)}ê°œ")
${testRatio > 0 ? `print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")` : ""}`;
      break;
    }

    case "training-config": {
      const epochs = nodeData.epochs || 3;
      const batchSize = nodeData.batchSize || 4;
      const learningRate = nodeData.learningRate || 5e-5;
      const warmupSteps = nodeData.warmupSteps || 500;
      const outputDir = nodeData.outputDir || "./output";

      code = `# í•™ìŠµ ì„¤ì •
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="${outputDir}",
    num_train_epochs=${epochs},
    per_device_train_batch_size=${batchSize},
    per_device_eval_batch_size=${batchSize},
    learning_rate=${learningRate},
    warmup_steps=${warmupSteps},
    logging_steps=${nodeData.loggingSteps || 10},
    save_strategy="${nodeData.saveStrategy || "epoch"}",
    eval_strategy="${nodeData.evalStrategy || "epoch"}",
    load_best_model_at_end=True,
    report_to="tensorboard",
    fp16=True,  # GPU ì‚¬ìš© ì‹œ
)

print("âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ")`;
      break;
    }

    case "lora-config": {
      const rank = nodeData.rank || 8;
      const alpha = nodeData.alpha || 16;
      const dropout = nodeData.dropout || 0.1;
      const targetModules = nodeData.targetModules || "q_proj,v_proj";

      code = `# LoRA ì„¤ì •
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=${rank},
    lora_alpha=${alpha},
    target_modules=[${targetModules.split(",").map((m: string) => `"${m.trim()}"`).join(", ")}],
    lora_dropout=${dropout},
    bias="${nodeData.bias || "none"}",
    task_type="CAUSAL_LM",
)

# LoRA ëª¨ë¸ ì ìš©
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

print("âœ… LoRA ì„¤ì • ì™„ë£Œ")`;
      break;
    }

    case "training": {
      code = `# ëª¨ë¸ í•™ìŠµ
from transformers import Trainer, DataCollatorForLanguageModeling

# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # Causal LM
)

# Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

# í•™ìŠµ ì‹œì‘
print("ğŸš€ í•™ìŠµ ì‹œì‘...")
train_result = trainer.train()

print("âœ… í•™ìŠµ ì™„ë£Œ!")
print(f"   ìµœì¢… Loss: {train_result.training_loss:.4f}")`;
      break;
    }

    case "model-saver": {
      const savePath = nodeData.savePath || "./saved_models";
      const saveFormat = nodeData.saveFormat || "both";

      code = `# ëª¨ë¸ ì €ì¥
save_path = "${savePath}"

if "${saveFormat}" == "both" or "${saveFormat}" == "peft":
    # LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
    model.save_pretrained(f"{save_path}/lora")
    tokenizer.save_pretrained(f"{save_path}/lora")
    print(f"âœ… LoRA ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {save_path}/lora")

if "${saveFormat}" == "both" or "${saveFormat}" == "merged":
    # ì „ì²´ ëª¨ë¸ ë³‘í•© ë° ì €ì¥
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(f"{save_path}/merged")
    tokenizer.save_pretrained(f"{save_path}/merged")
    print(f"âœ… ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}/merged")`;
      break;
    }

    default:
      code = `# ${node.type} ë…¸ë“œ
# ì´ ë…¸ë“œëŠ” ì•„ì§ ì½”ë“œ ìƒì„±ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
pass`;
  }

  return code;
}

/**
 * ì›Œí¬í”Œë¡œìš°ë¥¼ Jupyter Notebookìœ¼ë¡œ ë³€í™˜
 */
export function workflowToNotebook(nodes: Node[], connections: Connection[]): Notebook {
  // ìœ„ìƒ ì •ë ¬ë¡œ ë…¸ë“œ ì‹¤í–‰ ìˆœì„œ ê²°ì •
  const executedNodes = new Set<string>();
  const nodeDependencies = new Map<string, string[]>();
  const sortedNodes: Node[] = [];

  // ê° ë…¸ë“œì˜ ì˜ì¡´ì„± ê³„ì‚°
  nodes.forEach((node) => {
    const deps: string[] = [];
    connections.forEach((conn) => {
      if (conn.target === node.id) {
        deps.push(conn.source);
      }
    });
    nodeDependencies.set(node.id, deps);
  });

  // ìœ„ìƒ ì •ë ¬ ì‹¤í–‰
  while (executedNodes.size < nodes.length) {
    let progress = false;

    for (const node of nodes) {
      if (executedNodes.has(node.id)) continue;

      const deps = nodeDependencies.get(node.id) || [];
      const allDepsExecuted = deps.every((dep) => executedNodes.has(dep));

      if (allDepsExecuted) {
        progress = true;
        sortedNodes.push(node);
        executedNodes.add(node.id);
      }
    }

    if (!progress) {
      // ìˆœí™˜ ì˜ì¡´ì„± ë˜ëŠ” ë…ë¦½ ë…¸ë“œ ì²˜ë¦¬
      const remainingNodes = nodes.filter((n) => !executedNodes.has(n.id));
      remainingNodes.forEach((node) => {
        sortedNodes.push(node);
        executedNodes.add(node.id);
      });
      break;
    }
  }

  // Notebook ì…€ ìƒì„±
  const cells: NotebookCell[] = [];

  // í—¤ë” ë§ˆí¬ë‹¤ìš´ ì…€
  cells.push({
    cell_type: "markdown",
    source: [
      "# LLM Fine-tuning Pipeline\n",
      "\n",
      "ì´ ë…¸íŠ¸ë¶ì€ Mactuner ì›Œí¬í”Œë¡œìš°ì—ì„œ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì‹¤í–‰ ìˆœì„œ\n",
      "1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
      "2. í† í° ì„¤ì •\n",
      "3. ëª¨ë¸ ë° ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
      "4. ë°ì´í„° ì „ì²˜ë¦¬\n",
      "5. ëª¨ë¸ í•™ìŠµ\n",
      "6. ëª¨ë¸ ì €ì¥\n",
    ],
    metadata: {},
  });

  // ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì…€
  cells.push({
    cell_type: "code",
    source: [
      "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
      "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
      "!pip install -q huggingface_hub\n",
    ],
    metadata: {},
    execution_count: null,
    outputs: [],
  });

  // ê° ë…¸ë“œë³„ ì½”ë“œ ì…€ ìƒì„±
  const nodeOutputs: Record<string, any> = {};
  sortedNodes.forEach((node) => {
    const code = generateNodeCode(node, nodeOutputs);
    if (code.trim()) {
      cells.push({
        cell_type: "code",
        source: code.split("\n").map((line) => line + "\n"),
        metadata: {},
        execution_count: null,
        outputs: [],
      });
    }
  });

  // Notebook êµ¬ì¡° ìƒì„±
  const notebook: Notebook = {
    cells,
    metadata: {
      kernelspec: {
        display_name: "Python 3",
        language: "python",
        name: "python3",
      },
      language_info: {
        name: "python",
        version: "3.10.0",
      },
    },
    nbformat: 4,
    nbformat_minor: 4,
  };

  return notebook;
}

/**
 * Notebookì„ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
 */
export function downloadNotebook(notebook: Notebook, filename: string = "workflow.ipynb") {
  const json = JSON.stringify(notebook, null, 2);
  const blob = new Blob([json], { type: "application/json" });
  const url = URL.createObjectURL(blob);
  const link = document.createElement("a");
  link.href = url;
  link.download = filename;
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  URL.revokeObjectURL(url);
}

